{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# W207 Final Project : Facial Keypoint Detection \n",
    "# Team: Joanie Weaver, Sandip Panesar, Jackie Nichols, Rakesh Walisheter\n",
    "W207 Tuesday @4pm\n",
    "\n",
    "ref: https://www.kaggle.com/c/facial-keypoints-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Imports, reading in files, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615669714648
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import zlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_pickle():\n",
    "    train = pickle.load( open( \"data/train.p\", \"rb\" ) )\n",
    "    train.rename(columns = {'level_0' : 'index'}, inplace = True)\n",
    "    return train\n",
    "\n",
    "def load_train_dup_pickle():\n",
    "    train_duplicates = pickle.load( open(\"data/traindup.p\", \"rb\"))\n",
    "    train_duplicates.set_index('index', inplace=True, drop=False)\n",
    "    return train_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615679951578
    }
   },
   "outputs": [],
   "source": [
    "#Load the pickle files\n",
    "\n",
    "train = pickle.load( open( \"data/train.p\", \"rb\" ) )\n",
    "test = pickle.load( open(\"data/test.p\", \"rb\"))\n",
    "\n",
    "train.rename(columns = {'level_0' : 'index'}, inplace = True)\n",
    "\n",
    "train_duplicates = pickle.load( open(\"data/traindup.p\", \"rb\"))\n",
    "test_duplicates = pickle.load( open(\"data/testdup.p\", \"rb\"))\n",
    "\n",
    "train_duplicates.set_index('index', inplace=True, drop=False)\n",
    "print(\"Test shape is: \", test.shape)\n",
    "print(\"Train shape is: \", train.shape)\n",
    "\n",
    "print(\"Test duplicates shape is: \", test_duplicates.shape)\n",
    "print(\"Train duplicates shape is: \", train_duplicates.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615676047213
    }
   },
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS to reset the train and test dataframes\n",
    "\n",
    "def reset_train_df():\n",
    "    #train = df['train'].reset_index().copy()\n",
    "    new_train = train.reset_index().copy()\n",
    "    #Get the images and perform a checksum on every image in train: https://www.geeksforgeeks.org/zlib-adler32-in-python/\n",
    "    new_train['check_sum'] = train.image.map(lambda x: zlib.adler32(x))\n",
    "    new_train.pop('level_0')\n",
    "    return new_train\n",
    "\n",
    "def reset_test_df():\n",
    "    #test = df['test'].reset_index().copy()\n",
    "    new_test = test.reset_index().copy()\n",
    "    #Get the images and perform a checksum on every image in train: https://www.geeksforgeeks.org/zlib-adler32-in-python/\n",
    "    new_test['check_sum'] = test.image.map(lambda x: zlib.adler32(x))\n",
    "    new_test.pop('level_0')\n",
    "    return new_test\n",
    "\n",
    "def get_coordinate_columns():\n",
    "    coordinates = [c for c in train.columns if c.endswith('_x') | c.endswith('_y')]\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615679957351
    }
   },
   "outputs": [],
   "source": [
    "#Creating a copy of the train data in train_data in case you want to add columns back in from df[train]\n",
    "train_data=train.copy(deep=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Identification\n",
    "As we saw in the EDA, there are a variety of types of images with a variety of keypoints. Below, we will remove some of the images we saw as outliers in the EDA.\n",
    "\n",
    "Outlier types:\n",
    "- Mislabelled images\n",
    "- Weird/bad images\n",
    "- All outliers (i.e. images that contain keypoints that are greater than 3 standard deviations away from the mean for that keypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615675669204
    }
   },
   "outputs": [],
   "source": [
    "#CODE CELL FOR JOANIE\n",
    "\n",
    "#This block is identifying and counting all outliers\n",
    "#Outliers are images that contain keypoints > 3std from mean\n",
    "def find_outliers():\n",
    "    train=train_data.drop([\"image\"],axis=1)\n",
    "    described_train=train.describe().T\n",
    "    std=described_train[\"std\"]\n",
    "    mean=described_train[\"mean\"]\n",
    "    q1=described_train[\"25%\"]\n",
    "    q3=described_train[\"75%\"]\n",
    "    iqr=q3-q1\n",
    "\n",
    "    #If we define outliers using IQR\n",
    "    #outlier_low=q1-1.5*iqr\n",
    "    #outlier_high=q3+1.5*iqr\n",
    "\n",
    "    #If we define outliers using std\n",
    "    outlier_low=mean-3*std\n",
    "    outlier_high=mean+3*std\n",
    "\n",
    "\n",
    "\n",
    "    #Keep track of these images in a list\n",
    "    outlier_images=[]\n",
    "    outlier_dict={}\n",
    "\n",
    "    #Iterate through the data to find outliers based on whether they are lower/higher than defined outlier boundaries\n",
    "    for col in train.columns:\n",
    "        indices=list(np.where((train[col] < outlier_low[col]) | (train[col] > outlier_high[col]))[0])\n",
    "        outlier_images.extend(indices)\n",
    "        for i in indices:\n",
    "            temp=outlier_dict.get(i,[])\n",
    "            temp.append(col[:-1])\n",
    "            outlier_dict[i]=temp\n",
    "\n",
    "    #Only count each index once\n",
    "    outliers=np.unique(outlier_images)\n",
    "    outliers\n",
    "    print(\"Finding points 3 standard deviations away from the mean results in \",len(outliers),\n",
    "        \"images being classified as outliers\")\n",
    "    print(\"This represents\",len(outliers)/train.shape[0]*100,\"% of our total data\")\n",
    "\n",
    "    #print(train.shape)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615675673874
    }
   },
   "outputs": [],
   "source": [
    "#CODE CELL FOR JOANIE\n",
    "\n",
    "#This function is removing the worst outliers\n",
    "#The worst outliers are the mislabelled images and the weird/bad images\n",
    "def drop_worst_outliers():\n",
    "\n",
    "    print(\"Before dropping worst outliers train shape is: \", train.shape)\n",
    "    print(\"Before dropping worst outliers train duplicates shape is: \", train_duplicates.shape)\n",
    "    miss_labelled = [1747, 1877, 1907,2199] #These are the images with keypoints that are not right\n",
    "    bad_images = [6492,6493,2430,3697] #These are the two collages and the two cartoons\n",
    "\n",
    "    worst_outliers = miss_labelled + bad_images\n",
    "\n",
    "    #Drop with inplace drops inplace\n",
    "    #train_data.drop(index=worst_outliers,inplace=True)\n",
    "    train.drop(index=worst_outliers,inplace=True,errors='ignore')\n",
    "    train_duplicates.drop(index=worst_outliers,inplace=True,errors='ignore')\n",
    "    print(\"After dropping worst outliers train shape is: \", train.shape)\n",
    "    print(\"After dropping worst outliers train duplicates shape is: \", train_duplicates.shape)\n",
    "\n",
    "#This function is for removing all outliers as defined above\n",
    "def drop_all_outliers():\n",
    "    print(\"Before dropping all outliers train shape is: \", train.shape)\n",
    "    print(\"Before dropping all outliers train duplicates shape is: \", train_duplicates.shape)\n",
    "    \n",
    "    outliers = find_outliers()\n",
    "    overlap = [bad for bad in outliers if bad in train_duplicates.index]\n",
    "\n",
    "    train.drop(index=outliers,inplace=True, errors='ignore')\n",
    "    #Drop the overlap outliers and duplicates\n",
    "    train_duplicates.drop(index=overlap,inplace=True,errors='ignore')\n",
    "    print(\"After dropping all outliers train shape is: \", train.shape)\n",
    "    print(\"After dropping all outliers train duplicates shape is: \", train_duplicates.shape)\n",
    "\n",
    "def drop_overlap_outliers():\n",
    "    print(\"Before dropping overlap outliers train shape is: \", train.shape)\n",
    "    print(\"Before dropping overlap outliers train duplicates shape is: \", train_duplicates.shape)\n",
    "    \n",
    "    outliers = find_outliers()\n",
    "    overlap = [bad for bad in outliers if bad in train_duplicates.index]\n",
    "\n",
    "    print(\"There are\", len( overlap), \" images that are outliers that appear in train duplicates\")\n",
    "    \n",
    "    train.drop(index=overlap,inplace=True, errors='ignore')\n",
    "    #Drop the overlap outliers and duplicates\n",
    "    train_duplicates.drop(index=overlap,inplace=True,errors='ignore')\n",
    "    print(\"After dropping overlap outliers train shape is: \", train.shape)\n",
    "    print(\"After dropping overlap outliers train duplicates shape is: \", train_duplicates.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code for Duplicate Data in Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615680029884
    }
   },
   "outputs": [],
   "source": [
    "#CODE CELL FOR JACKIE\n",
    "# Remove duplicates in the train dataset by taking the mean of all values for that image in each label \n",
    "def remove_train_duplicates( train_duplicates,verbose=True):\n",
    "    # First let's reset the index since we've been working on the df \n",
    "        \n",
    "    train = reset_train_df()\n",
    "    #train_duplicates.reset_index()\n",
    "\n",
    "    #Get all of the coordinates\n",
    "    coordinates = get_coordinate_columns()\n",
    "\n",
    "    #Create an empty df with the coordinate columns in place\n",
    "    final_images = train[(train.index == -1)][coordinates].copy()\n",
    "    final_check_sum = train_duplicates.check_sum.unique()\n",
    "\n",
    "    #For each unique check_sum in duplicates...\n",
    "    for check_sum in train_duplicates.check_sum.unique():\n",
    "        #Get all of the duplicates with the same check_sum\n",
    "        duplicates = train_duplicates[(train_duplicates.check_sum == check_sum)]['index'].values\n",
    "        \n",
    "        #Get the first image that appears in the train dataset with this check_sum\n",
    "        image = train[(train['index'].isin(duplicates))].image.values[0]\n",
    "        #Take the mean of all the coordinate columns - this is what we will use for the final single image\n",
    "        fixed = pd.DataFrame(pd.DataFrame(train[(train['index'].isin(duplicates))], columns=coordinates).mean(axis = 0)).T\n",
    "        #Make sure to include the actual image (lol)\n",
    "        fixed['image'] = [image]\n",
    "        fixed['check_sum'] = check_sum\n",
    "        fixed['index'] = duplicates[0] #take first index\n",
    "        \n",
    "        #Append it to the list of final_images\n",
    "        final_images = final_images.append(fixed, ignore_index = True)\n",
    "        \n",
    "            \n",
    "    #For reporting purposes: \n",
    "    if verbose: print(\"=\"*13 + \"Train\" + \"=\"*13)\n",
    "    if verbose: print(\"Before delete:     %s\" % str(train.shape))\n",
    "\n",
    "    #Remove the duplicates from train - danger, danger, must replace them\n",
    "    train = train[~(train['index'].isin(train_duplicates['index'].values))]\n",
    "    if verbose: print(\"After  delete:     %s\" % str(train.shape))\n",
    "\n",
    "    #Dump the final images that were duplicates so we can take a look at them after the processing. \n",
    "    pickle.dump(final_check_sum, open( \"data/final_check_sum.p\", \"wb\" ) )\n",
    " \n",
    "\n",
    "    #Replace removed duplicates with final_images\n",
    "    train = train.append(final_images, ignore_index = True).reset_index()\n",
    "    train['check_sum'] = train.image.map(lambda x: zlib.adler32(x))\n",
    "    #train.drop(columns=['index'], inplace = True)\n",
    "    if verbose: print(\"After  append:     %s\" % str(train.shape))\n",
    "    return train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615674308590
    }
   },
   "outputs": [],
   "source": [
    "#CODE CELL FOR JACKIE\n",
    "##########Test Data set\n",
    "\n",
    "#Now do the same for test, this will be easier since we don't need\n",
    "#to deal with points and taking the mean\n",
    "def remove_test_duplicates(verbose=True):\n",
    "#We can do this differently since we don't need to take the mean. \n",
    "#Go through the test and only add items to the final test image if\n",
    "#we do not already have the check_sum. If we find the check_sum, don't\n",
    "#add it it's a duplicate. \n",
    "    test = reset_test_df()\n",
    "    if verbose: print(\"=\"*13 + \"Test=\" + \"=\"*13)\n",
    "    if verbose: print(\"Before delete:     %s\" % str(test.shape))\n",
    "    test = reset_test_df()\n",
    "    #Create an empty df with the coordinate columns in place\n",
    "    final_test_images = test[(test.index == -1)]\n",
    "    \n",
    "    for test_index, check_sum in zip(test['index'], test.check_sum):\n",
    "        if not (check_sum in list(final_test_images.check_sum.values)):\n",
    "            final_test_images = final_test_images.append(test.loc[(test['index'] == test_index)], ignore_index = True)\n",
    "    \n",
    "    if verbose: print(\"After  delete:     %s\" % str(final_test_images.shape))\n",
    "    return final_test_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PICKLE DIFFERENT VERSIONS OF TRAIN CLEANED\n",
    "In order for this to work you will need to reload the orginal pickle files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615679966335
    }
   },
   "outputs": [],
   "source": [
    "#PICKLE 1: OVERLAP OUTLIERS\n",
    "#only drop the overlap outliers for now\n",
    "train = load_train_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "drop_overlap_outliers()\n",
    "pickle.dump( train, open( \"cleantrain/clean_o_outliers.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE CELL FOR JACKIE\n",
    "#PICKLE 2: WORST OUTLIERS\n",
    "train = load_train_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "drop_worst_outliers()\n",
    "#Pickle train so that we can jump in with cleaning this data\n",
    "pickle.dump( train, open( \"cleantrain/clean_w_outliers.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKLE 3: ALL OUTLIERS\n",
    "train = load_train_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "drop_overlap_outliers()\n",
    "drop_worst_outliers()\n",
    "pickle.dump( train, open( \"cleantrain/clean_all_outliers.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE PICKLES WITH duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615680001628
    }
   },
   "outputs": [],
   "source": [
    "#CODE CELL FOR JACKIE\n",
    "#PICKLE 4: DUPLICATES ONLY\n",
    "#Only run this cell to get clean_duplicates\n",
    "print(\"Applying EDA fix for duplicates\")\n",
    "print()\n",
    "train = load_train_pickle()\n",
    "train_duplicates = load_train_dup_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "train = remove_train_duplicates(train_duplicates)\n",
    "pickle.dump( train, open( \"cleantrain/clean_duplicates.p\", \"wb\" ) )\n",
    "print()\n",
    "test = remove_test_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE CELL FOR JACKIE\n",
    "#PICKLE 5: OVERLAP OUTLIERS + DUPLICATES\n",
    "#Overlap outliers + duplicates\n",
    "train = load_train_pickle()\n",
    "train_duplicates = load_train_dup_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "drop_overlap_outliers()\n",
    "train = remove_train_duplicates(train_duplicates)\n",
    "pickle.dump( train, open( \"cleantrain/clean_o_dups.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKLE 6: WORST OUTLIERS + DUPLICATES\n",
    "# Worst + DUPS\n",
    "train = load_train_pickle()\n",
    "train_duplicates = load_train_dup_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "drop_worst_outliers()\n",
    "train = remove_train_duplicates(train_duplicates)\n",
    "pickle.dump( train, open( \"cleantrain/clean_w_dups.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKLE 7: OVERLAP + WORST OUTLIERS + DUPLICATES\n",
    "#Overlap outliers + duplicates\n",
    "train = load_train_pickle()\n",
    "train_duplicates = load_train_dup_pickle()\n",
    "train_data=train.copy(deep=True)\n",
    "drop_overlap_outliers()\n",
    "drop_worst_outliers()\n",
    "train = remove_train_duplicates(train_duplicates)\n",
    "pickle.dump( train, open( \"cleantrain/clean_wo_dups.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIEW SOME IMAGES WITH MEAN APPLIED AFTER DUPLICATE REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615680080619
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Load the fixed images from the duplication process\n",
    "fixed_images = pickle.load( open( \"data/final_check_sum.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615680087211
    }
   },
   "outputs": [],
   "source": [
    "#Print clean images:\n",
    "\n",
    "def show_fixed_images(fixed_images):\n",
    "    ## TRAIN \n",
    "    # Let's view some of these duplicated train images\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.suptitle('Sample of Cleaned Duplicate images from the Train dataset\\n n= 50', size = 20,  y = 1.04, weight = 'bold')\n",
    "    #Get the point coordinates for example: mouth_center_top_lip_x\n",
    "    coordinates = get_coordinate_columns()\n",
    "    #print(coordinates)\n",
    "    \n",
    "   \n",
    "    match_pts = pd.DataFrame(columns =['Points_Found', 'Count'])\n",
    "\n",
    "     #Loop through and plot each of the 50 images.  \n",
    "    for i, check_sum in enumerate (fixed_images):\n",
    "        if i == 50:\n",
    "            break\n",
    "        plt.subplot(10,5,i+1)\n",
    "        img = train[(train['check_sum'] == check_sum)].image.values[0].reshape(96,96)\n",
    "        #These are the points that have been identified on the images\n",
    "        points = train[(train['check_sum'] == check_sum)][coordinates].values[0]\n",
    "        idx = train[(train['check_sum'] == check_sum)]['index']\n",
    "        plt.imshow(img, cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "        matching_pts = 0\n",
    "\n",
    "        for pts in range(0, 30, 2):\n",
    "            x_point, y_point = (points[pts], points[pts+1])\n",
    "            if not (np.isnan(x_point)) and not (np.isnan(y_point)):\n",
    "                matching_pts += 1\n",
    "                #Add the point to the plot\n",
    "                plt.plot(x_point, y_point, 'o', color = \"red\", markersize = 5)\n",
    "                \n",
    "\n",
    "        plt.title(\"Image #:[%d]\\n#Points:[%d]\" % (idx, matching_pts))\n",
    "        if matching_pts in match_pts[\"Points_Found\"].values:\n",
    "                match_pts.loc[match_pts['Points_Found'] == matching_pts, 'Count'] = match_pts.loc[match_pts['Points_Found'] == matching_pts, 'Count'] + 1\n",
    "        else:\n",
    "            match_pts = match_pts.append({'Points_Found':matching_pts,'Count': 1},ignore_index=True)\n",
    "\n",
    "\n",
    "    #We should save every image with a marker so we can look at them\n",
    "    #im = Image.fromarray(img)\n",
    "    #im.save(file_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1615680095849
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "show_fixed_images(fixed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the IDLookup table and pickle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lookup = pd.read_csv(\"data/IdLookupTable.csv\", names = ['row_id', 'image_id', 'feature_name', 'location'], dtype = {'row_id':'uint16', 'image_id':'uint16', 'location':'float32'}, skiprows = 1)\n",
    "\n",
    "print(\"Creating lookup pickle file id_lookup.p\")\n",
    "pickle.dump(id_lookup, open(\"data/id_lookup.p\", \"wb\"))\n",
    "print(\"Pickle creation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
