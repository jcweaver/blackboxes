{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Augment_Missing_Data.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernel_info":{"name":"python3-azureml"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"nteract":{"version":"nteract-front-end@1.0.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lhx7zxedo6rX","nteract":{"transient":{"deleting":false}}},"source":["# W207 Final Project : Facial Keypoint Detection \n","# Team: Joanie Weaver, Sandip Panesar, Jackie Nichols, Rakesh Walisheter\n","W207 Tuesday @4pm\n","\n","ref: https://www.kaggle.com/c/facial-keypoints-detection"]},{"cell_type":"code","metadata":{"gather":{"logged":1615317258474},"id":"RbpFWm1Go6rn","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","\n","from tqdm import tqdm\n","import zlib\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from matplotlib import rc\n","from matplotlib.ticker import PercentFormatter\n","import pickle\n","from  sklearn.linear_model import LinearRegression\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"gather":{"logged":1615317192680},"id":"ofmd6msto6rr","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":["#Load the pickle files\n","\n","train_data = pickle.load( open( \"cleantrain/clean_all_outliers.p\", \"rb\" ) )\n","train_data = train_data.reset_index(drop=True)\n","train_data.rename(columns = {'level_0' : 'index'}, inplace = True)\n","\n","\n","print(\"Train shape is: \", train_data.shape)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gWeniBWmo6rt","nteract":{"transient":{"deleting":false}}},"source":["# Potential Approach for missing data\n","# Filling In Missing Data Using Linear-Models\n","\n","Notes:\n","\n","* 'y' : An incomplete feature in the data set which is being augmented\n","\n","* 'X' : a collection of features which are dense in data and are highly-correlated to 'y'.\n","\n","* Features which have no more than 50-missing data-points are considered `Dense`. Features which have no missing-data-points are considered `full`.\n","\n","* Augmentation considers atleast 2-reference points which are `significantly correlated` to augment any feature; this is to have a `triangulation` in the image rather than just depend on one point.\n","\n","* Correlations of more than 0.5 are considered as `significant correlations`. The corresponding-features are earmarked to be used to triangulate a predicted location.\n","\n","* Data-points which are full in ('X' and 'y') are used to train the corresponding linear-model.\n","\n","* Data-points which are full in 'X' but empty in 'y' are augmented by this model.\n","\n","* This augmentation was possible by setting the acceptable accuracy (R^2) of the linear-models generated to a minimum-acceptable-score of 45% accurate.. any higher than this and the augmentation does not converge; for a few features, the models that came up were less than 50% accurate.\n"]},{"cell_type":"code","metadata":{"gather":{"logged":1615317210055},"id":"5MnniUvJo6rv","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":["\n","# Fetch the most significantly-correlated features for each feature in the `data_under_cleansing` set.\n","# get_feature_correlations returns a dict of feat_name:Series<FeatName:Correlations>\n","def get_feature_correlations(data_under_cleansing, low=0.5, high=1):\n","    correlations = data_under_cleansing.corr()\n","    max_correlations = correlations[(correlations>low) & (correlations<high)]\n","    feature_corrs = {}\n","    for column in max_correlations:\n","        corr_scores = max_correlations[column]\n","        significant_correlations = corr_scores.dropna()\n","        feature_corrs[significant_correlations.name]=significant_correlations\n","    return feature_corrs\n","\n","\n","# In the data set `data_under_cleansing`, this method looks for features which do not have more than 50 missing data-values.\n","# returns a bool-mask representing : <feature> :: <bool- is data dense>\n","def get_data_density_mask(data_under_cleansing):\n","    features = data_under_cleansing.columns\n","    data_under_cleansing_mask = {}\n","    for i in features:\n","        missing_count = sum(data_under_cleansing[i].isna())\n","        data_under_cleansing_mask[i] = missing_count<50\n","    return data_under_cleansing_mask\n","\n","\n","# Method to run the augmentation on given data.\n","def do_augment_missing_data(data_under_cleansing, density_mask, plot_correlations, reference_point_count=2, min_model_score=0.35):\n","    feat_corrs = get_feature_correlations(data_under_cleansing)\n","\n","    #all feature-correlations for features which are reported as not dense\n","    all_features_to_augment = [feat_corrs[feature] for feature in density_mask if not density_mask[feature]]\n","\n","    for feature_data in all_features_to_augment:\n","        # Do this for each feature that needs to be augmented due to large missing values\n","        # get_feature_correlations returns a dict of feat_name:Series<FeatName:Correlations>\n","        # name here gives the name of the extracted series.\n","        feat_to_be_augmented = feature_data.name\n","        \n","        high_corr_full_features = [feat for feat in feature_data.index.tolist() if density_mask[feat]]\n","        if len(high_corr_full_features) < reference_point_count:\n","            continue\n","        \n","        #\"filtering train-data set where all high-corr-features and feat-to-be-augmented are not-NA\"\n","        query_str_train = ' & '.join(['~{}.isna()'.format(k) for k in high_corr_full_features])\n","        query_str_train = ' & '.join([query_str_train, '~{}.isna()'.format(feat_to_be_augmented)])\n","        #print(query_str_train)\n","\n","        tmp_train_data  = data_under_cleansing.query(query_str_train,engine=\"python\")\n","        tmp_train_X = tmp_train_data[high_corr_full_features]\n","        tmp_train_y = tmp_train_data[feat_to_be_augmented]\n","        \n","        if plot_correlations:\n","            print(\"Plotting y against each X.... \\n\\n \")\n","            for x in high_corr_full_features:\n","                tmp_train_data.plot(x=x, y=feat_to_be_augmented, style='o')\n","                plt.show()\n","\n","        #\"filtering predict-data set where all high-corr-features are not-NA and feat-to-be-augmented are NA\"\n","        query_str_predict = ' & '.join(['~{}.isna()'.format(k) for k in high_corr_full_features])\n","        query_str_predict = ' & '.join([query_str_predict, '{}.isna()'.format(feat_to_be_augmented)])\n","        \n","        \n","        tmp_predict_data  = data_under_cleansing.query(query_str_predict,engine=\"python\")\n","        tmp_predict_X = tmp_predict_data[high_corr_full_features]\n","        lm = LinearRegression().fit(tmp_train_X, tmp_train_y)\n","        model_score =  lm.score(tmp_train_X, tmp_train_y)\n","        print(\"Model score: \", model_score)\n","        if model_score < min_model_score:\n","            # do not use a model to augment data when model is less than 45% accurate. Shifting this threshold to 50% leads to NON-CONVERGENCE\n","            print(\"aborting augmenting..\")\n","            continue\n","\n","        print(\"Model coef: \" , lm.coef_)\n","        tmp_predict_y = list(lm.predict(tmp_predict_X))\n","        feat_column_index = data_under_cleansing.columns.get_loc(feat_to_be_augmented)\n","        index_list = tmp_predict_data.index.tolist()\n","\n","        for i, index in enumerate(index_list):\n","            data_under_cleansing.iloc[index][feat_column_index] = tmp_predict_y[i]\n","\n","    return data_under_cleansing\n","\n","\n","def augment_missing_data(given_dataset, plot_correlations=False, allow_reduced_ref_point=False, min_acceptable_lm_score=0.35):\n","    '''\n","    Utility Method which takes a data set of size `n` with `m`-features and augments \n","    features which are missing using linear-regression models. There is not guarantee that \n","    this augmentation process will converge for all data-sets. But it is known to work for the train-data\n","    from this project.\n","    '''\n","    images = given_dataset['image']\n","    rogue_columns = ['image', 'index', 'check_sum']\n","    data_to_be_cleansed = given_dataset[given_dataset.columns.difference(rogue_columns)]\n","    print(data_to_be_cleansed.shape)\n","\n","    incomplete_feature_count = -1\n","\n","    while True:\n","        print(\"\\n\\n==========================================================\")\n","        data_density_mask = get_data_density_mask(data_to_be_cleansed)\n","        incomplete_features = [key for key in data_density_mask.keys() if not data_density_mask[key]]\n","        complete_features    = [key for key in data_density_mask.keys() if data_density_mask[key]]\n","\n","        print(\"Incomplete Features: \", len(incomplete_features))\n","        print(\"Complete Features: \", len(complete_features))\n","\n","        # a feature_threshold to identify how many features are to be used to model \n","        # feature being augmented. Minimum is 2.\n","        reference_point_count = 2\n","\n","        if len(incomplete_features) > 0:\n","            if incomplete_feature_count == len(incomplete_features):\n","              if allow_reduced_ref_point:\n","                if reference_point_count > 1:\n","                  print(\"Model is not converging; reducing reference-point-count to allow convergence.\")\n","                  reference_point_count = 1\n","                else:\n","                  raise Exception(\"Error: Augmentation not converging; Reference-points inadequate; try to SET allow_reduced_ref_point=TRUE if not already\")\n","            else:\n","              incomplete_feature_count = len(incomplete_features)\n","              \n","            data_to_be_cleansed = do_augment_missing_data(data_to_be_cleansed, data_density_mask, plot_correlations, reference_point_count, min_acceptable_lm_score)\n","        else:\n","            break\n","    \n","    # Add back the image column\n","    data_to_be_cleansed = data_to_be_cleansed.assign(image=images)\n","    return data_to_be_cleansed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JA03NbIKo6r3","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"source":["#CODE CELL FOR RAKESH\n","\n","## Pass in the right data-set to augment and get the augmented data back (filtering 'image' column here)\n","augmented_data = augment_missing_data(train_data, allow_reduced_ref_point=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSrERCqQo6r7","nteract":{"transient":{"deleting":false}}},"source":["## Save Augmented Data to a Pickle file"]},{"cell_type":"code","metadata":{"id":"WOB65zZ2o6r7","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":[]},"source":["#CODE CELL FOR JACKIE\n","# Pickle train and test so that we can jump in with cleaning this data\n","# pickle.dump( augmented_data, open( \"data/aug_train.p\", \"wb\" ) )\n","import os\n","files = os.listdir(\"CleanTrain\")\n","\n","for filename in files:\n","    print(\"Opening file: \", filename)\n","    clean_file = \"\".join((\"CleanTrain/\",filename))\n","    train_data = pickle.load( open( clean_file, \"rb\" ) )\n","    train_data = train_data.reset_index(drop=True)\n","    train_data.rename(columns = {'level_0' : 'index'}, inplace = True)\n","    print(\"Train shape is: \", train_data.shape)\n","    print(\"Augmenting Data\")\n","    \n","    augmented_data = augment_missing_data(train_data, allow_reduced_ref_point=True)\n","    print(\"Augmented data shape: \", augmented_data.shape)\n","\n","    print(\"Pickle Augmented file\")\n","    aug_file = \"\".join((\"augmented/aug_\",filename))\n","    #Pickle train and test so that we can jump in with cleaning this data\n","    pickle.dump( augmented_data, open( aug_file, \"wb\" ) )\n","    print()"],"execution_count":null,"outputs":[]}]}