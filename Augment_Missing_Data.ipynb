{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# W207 Final Project : Facial Keypoint Detection \r\n",
        "# Team: Joanie Weaver, Sandip Panesar, Jackie Nichols, Rakesh Walisheter\r\n",
        "W207 Tuesday @4pm\r\n",
        "\r\n",
        "ref: https://www.kaggle.com/c/facial-keypoints-detection"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import warnings\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "import zlib\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "from matplotlib import rc\r\n",
        "from matplotlib.ticker import PercentFormatter\r\n",
        "import pickle\r\n",
        "from  sklearn.linear_model import LinearRegression\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615317258474
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the pickle files\r\n",
        "\r\n",
        "train_data = pickle.load( open( \"data/clean_train.p\", \"rb\" ) )\r\n",
        "\r\n",
        "train_data.rename(columns = {'level_0' : 'index'}, inplace = True)\r\n",
        "\r\n",
        "\r\n",
        "print(\"Train shape is: \", train_data.shape)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615317192680
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Approach for missing data\r\n",
        "# Filling In Missing Data Using Linear-Models\r\n",
        "\r\n",
        "Notes:\r\n",
        "\r\n",
        "* 'y' : An incomplete feature in the data set which is being augmented\r\n",
        "\r\n",
        "* 'X' : a collection of features which are dense in data and are highly-correlated to 'y'.\r\n",
        "\r\n",
        "* Features which have no more than 50-missing data-points are considered `Dense`. Features which have no missing-data-points are considered `full`.\r\n",
        "\r\n",
        "* Augmentation considers atleast 2-reference points which are `significantly correlated` to augment any feature; this is to have a `triangulation` in the image rather than just depend on one point.\r\n",
        "\r\n",
        "* Correlations of more than 0.5 are considered as `significant correlations`. The corresponding-features are earmarked to be used to triangulate a predicted location.\r\n",
        "\r\n",
        "* Data-points which are full in ('X' and 'y') are used to train the corresponding linear-model.\r\n",
        "\r\n",
        "* Data-points which are full in 'X' but empty in 'y' are augmented by this model.\r\n",
        "\r\n",
        "* This augmentation was possible by setting the acceptable accuracy (R^2) of the linear-models generated to a minimum-acceptable-score of 45% accurate.. any higher than this and the augmentation does not converge; for a few features, the models that came up were less than 50% accurate.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR RAKESH\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Fetch the most significantly-correlated features for each feature in the `data_under_cleansing` set.\r\n",
        "def get_feature_correlations(data_under_cleansing):\r\n",
        "    correlations = data_under_cleansing.corr()\r\n",
        "    max_correlations = correlations[(correlations>0.5) & (correlations<1)]\r\n",
        "    feature_corrs = {}\r\n",
        "    for column in max_correlations:\r\n",
        "        corr_scores = max_correlations[column]\r\n",
        "        significant_correlations = corr_scores.dropna()\r\n",
        "        feature_corrs[significant_correlations.name]=significant_correlations\r\n",
        "    return feature_corrs\r\n",
        "\r\n",
        "\r\n",
        "# In the data set `data_under_cleaning`, this method looks for features which do not have more than 50 missing data-values.\r\n",
        "# returns a bool-mask representing : <feature> :: <bool? is data dense>\r\n",
        "def get_data_density_mask(data_under_cleansing):\r\n",
        "    features = data_under_cleansing.columns\r\n",
        "    data_under_cleansing_mask = {}\r\n",
        "    for i in features:\r\n",
        "        missing_count = sum(data_under_cleansing[i].isna())\r\n",
        "        data_under_cleansing_mask[i] = missing_count<50\r\n",
        "    return data_under_cleansing_mask\r\n",
        "\r\n",
        "# Method to run the augmentation on given data.\r\n",
        "def do_augment_missing_data(data_under_cleansing, density_mask, plot_correlations):\r\n",
        "    feat_corrs = get_feature_correlations(data_under_cleansing)\r\n",
        "    print(\"Complete Features: \", len([key for key in density_mask.keys() if density_mask[key]]))\r\n",
        "\r\n",
        "    #all feature-correlations for features which are reported as not dense\r\n",
        "    all_features_to_augment = [feat_corrs[feature] for feature in density_mask if not density_mask[feature]]\r\n",
        "\r\n",
        "    for feature_data in all_features_to_augment:\r\n",
        "        # Do this for each feature that needs to be augmented due to large missing values\r\n",
        "        feat_to_be_augmented = feature_data.name\r\n",
        "        \r\n",
        "        high_corr_full_features = [feat for feat in feature_data.index.tolist() if density_mask[feat]]\r\n",
        "        if len(high_corr_full_features) < 2:\r\n",
        "            # a feature_threshold to identify how many features are to be used to model \r\n",
        "            # feature being augmented. Minimum is 2.\r\n",
        "            continue\r\n",
        "        \r\n",
        "        print(\"\\nfeat ..\", feat_to_be_augmented)\r\n",
        "        print(\"corr ..\", high_corr_full_features)\r\n",
        "\r\n",
        "        #\"filtering train-data set where all high-corr-features and feat-to-be-augmented are not-NA\"\r\n",
        "        query_str_train = ' & '.join(['~{}.isna()'.format(k) for k in high_corr_full_features])\r\n",
        "        query_str_train = ' & '.join([query_str_train, '~{}.isna()'.format(feat_to_be_augmented)])\r\n",
        "        #print(query_str_train)\r\n",
        "        tmp_train_data  = data_under_cleansing.query(query_str_train,engine=\"python\")\r\n",
        "        tmp_train_X = tmp_train_data[high_corr_full_features]\r\n",
        "        tmp_train_y = tmp_train_data[feat_to_be_augmented]\r\n",
        "        \r\n",
        "        if plot_correlations:\r\n",
        "            print(\"Plotting y against each X.... \\n\\n \")\r\n",
        "            for x in high_corr_full_features:\r\n",
        "                tmp_train_data.plot(x=x, y=feat_to_be_augmented, style='o')\r\n",
        "                plt.show()\r\n",
        "\r\n",
        "        #\"filtering predict-data set where all high-corr-features are not-NA and feat-to-be-augmented are NA\"\r\n",
        "        query_str_predict = ' & '.join(['~{}.isna()'.format(k) for k in high_corr_full_features])\r\n",
        "        query_str_predict = ' & '.join([query_str_predict, '{}.isna()'.format(feat_to_be_augmented)])\r\n",
        "        \r\n",
        "    \r\n",
        "        tmp_predict_data  = data_under_cleansing.query(query_str_predict,engine=\"python\")\r\n",
        "        tmp_predict_X = tmp_predict_data[high_corr_full_features]\r\n",
        "\r\n",
        "        lm = LinearRegression().fit(tmp_train_X, tmp_train_y)\r\n",
        "        model_score =  lm.score(tmp_train_X, tmp_train_y)\r\n",
        "        print(\"Model score: \", model_score)\r\n",
        "        if model_score < 0.45:\r\n",
        "            # do not use a model to augment data when model is less than 45% accurate. Shifting this threshold to 50% leads to NON-CONVERGENCE\r\n",
        "            print(\"aborting augmenting..\")\r\n",
        "            continue\r\n",
        "\r\n",
        "        print(\"Model coef: \" , lm.coef_)\r\n",
        "        tmp_predict_y = list(lm.predict(tmp_predict_X))\r\n",
        "        feat_column_index = data_under_cleansing.columns.get_loc(feat_to_be_augmented)\r\n",
        "        index_list = tmp_predict_data.index.tolist()\r\n",
        "\r\n",
        "        for i, index in enumerate(index_list):\r\n",
        "            data_under_cleansing.iloc[index][feat_column_index] = tmp_predict_y[i]\r\n",
        "\r\n",
        "    return data_under_cleansing\r\n",
        "\r\n",
        "\r\n",
        "def augment_missing_data(given_dataset, plot_correlations=False):\r\n",
        "    '''\r\n",
        "    Utility Method which takes a data set of size `n` with `m`-features and augments \r\n",
        "    features which are missing using linear-regression models. There is not guarantee that \r\n",
        "    this augmentation process will converge for all data-sets. But it is known to work for the train-data\r\n",
        "    from this project.\r\n",
        "    '''\r\n",
        "    data_to_be_cleansed = given_dataset.loc[:, given_dataset.columns != 'Image']\r\n",
        "    \r\n",
        "    while True:\r\n",
        "        print(\"\\n\\n==========================================================\")\r\n",
        "        data_density_mask = get_data_density_mask(data_to_be_cleansed)\r\n",
        "        incomplete_features = [key for key in data_density_mask.keys() if not data_density_mask[key]]\r\n",
        "        print(\"Incomplete Features: \", len(incomplete_features))\r\n",
        "        if len(incomplete_features) > 0:\r\n",
        "            data_to_be_cleansed = do_augment_missing_data(data_to_be_cleansed, data_density_mask, plot_correlations)\r\n",
        "        else:\r\n",
        "            break\r\n",
        "    \r\n",
        "    return data_to_be_cleansed"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1615317210055
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR RAKESH\r\n",
        "\r\n",
        "## Pass in the right data-set to augment and get the augmented data back (filtering 'image' column here)\r\n",
        "augmented_data = augment_missing_data(train_data.loc[:, train_data.columns != 'image'])\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Augmented Data to a Pickle file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR JACKIE\r\n",
        "print(augmented_data.shape)\r\n",
        "\r\n",
        "#Pickle train and test so that we can jump in with cleaning this data\r\n",
        "pickle.dump( augmented_data, open( \"data/aug_train.p\", \"wb\" ) )\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}