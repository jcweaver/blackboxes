{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit (conda)","metadata":{"interpreter":{"hash":"d230cf8de72e867e5717d0f5cf531d71189c7e5bd77bc2a42cd05122b296a561"}}},"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"},"colab":{"name":"Augment_Missing_Data.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"lhx7zxedo6rX"},"source":["# W207 Final Project : Facial Keypoint Detection \n","# Team: Joanie Weaver, Sandip Panesar, Jackie Nichols, Rakesh Walisheter\n","W207 Tuesday @4pm\n","\n","ref: https://www.kaggle.com/c/facial-keypoints-detection"]},{"cell_type":"code","metadata":{"id":"M8a7YtoapFWg"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SiVPEyBypL6h"},"source":["cd drive/MyDrive/w207/blackboxes/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1615317258474},"id":"RbpFWm1Go6rn"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","\n","from tqdm import tqdm\n","import zlib\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from matplotlib import rc\n","from matplotlib.ticker import PercentFormatter\n","import pickle\n","from  sklearn.linear_model import LinearRegression\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1615317192680},"id":"ofmd6msto6rr"},"source":["#Load the pickle files\n","\n","train_data = pickle.load( open( \"cleantrain/clean_all_outliers.p\", \"rb\" ) )\n","train_data = train_data.reset_index(drop=True)\n","train_data.rename(columns = {'level_0' : 'index'}, inplace = True)\n","\n","\n","print(\"Train shape is: \", train_data.shape)\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape is:  (7041, 33)\n"]}]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"gWeniBWmo6rt"},"source":["# Potential Approach for missing data\n","# Filling In Missing Data Using Linear-Models\n","\n","Notes:\n","\n","* 'y' : An incomplete feature in the data set which is being augmented\n","\n","* 'X' : a collection of features which are dense in data and are highly-correlated to 'y'.\n","\n","* Features which have no more than 50-missing data-points are considered `Dense`. Features which have no missing-data-points are considered `full`.\n","\n","* Augmentation considers atleast 2-reference points which are `significantly correlated` to augment any feature; this is to have a `triangulation` in the image rather than just depend on one point.\n","\n","* Correlations of more than 0.5 are considered as `significant correlations`. The corresponding-features are earmarked to be used to triangulate a predicted location.\n","\n","* Data-points which are full in ('X' and 'y') are used to train the corresponding linear-model.\n","\n","* Data-points which are full in 'X' but empty in 'y' are augmented by this model.\n","\n","* This augmentation was possible by setting the acceptable accuracy (R^2) of the linear-models generated to a minimum-acceptable-score of 45% accurate.. any higher than this and the augmentation does not converge; for a few features, the models that came up were less than 50% accurate.\n"]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1615317210055},"id":"5MnniUvJo6rv"},"source":["\n","# Fetch the most significantly-correlated features for each feature in the `data_under_cleansing` set.\n","# get_feature_correlations returns a dict of feat_name:Series<FeatName:Correlations>\n","def get_feature_correlations(data_under_cleansing, low=0.5, high=1):\n","    correlations = data_under_cleansing.corr()\n","    max_correlations = correlations[(correlations>low) & (correlations<high)]\n","    feature_corrs = {}\n","    for column in max_correlations:\n","        corr_scores = max_correlations[column]\n","        significant_correlations = corr_scores.dropna()\n","        feature_corrs[significant_correlations.name]=significant_correlations\n","    return feature_corrs\n","\n","\n","# In the data set `data_under_cleansing`, this method looks for features which do not have more than 50 missing data-values.\n","# returns a bool-mask representing : <feature> :: <bool- is data dense>\n","def get_data_density_mask(data_under_cleansing):\n","    features = data_under_cleansing.columns\n","    data_under_cleansing_mask = {}\n","    for i in features:\n","        missing_count = sum(data_under_cleansing[i].isna())\n","        data_under_cleansing_mask[i] = missing_count<50\n","    return data_under_cleansing_mask\n","\n","\n","# Method to run the augmentation on given data.\n","def do_augment_missing_data(data_under_cleansing, density_mask, plot_correlations, reference_point_count=2, min_model_score=0.35):\n","    feat_corrs = get_feature_correlations(data_under_cleansing)\n","\n","    #all feature-correlations for features which are reported as not dense\n","    all_features_to_augment = [feat_corrs[feature] for feature in density_mask if not density_mask[feature]]\n","\n","    for feature_data in all_features_to_augment:\n","        # Do this for each feature that needs to be augmented due to large missing values\n","        # get_feature_correlations returns a dict of feat_name:Series<FeatName:Correlations>\n","        # name here gives the name of the extracted series.\n","        feat_to_be_augmented = feature_data.name\n","        \n","        high_corr_full_features = [feat for feat in feature_data.index.tolist() if density_mask[feat]]\n","        if len(high_corr_full_features) < reference_point_count:\n","            continue\n","        \n","        #\"filtering train-data set where all high-corr-features and feat-to-be-augmented are not-NA\"\n","        query_str_train = ' & '.join(['~{}.isna()'.format(k) for k in high_corr_full_features])\n","        query_str_train = ' & '.join([query_str_train, '~{}.isna()'.format(feat_to_be_augmented)])\n","        #print(query_str_train)\n","\n","        tmp_train_data  = data_under_cleansing.query(query_str_train,engine=\"python\")\n","        tmp_train_X = tmp_train_data[high_corr_full_features]\n","        tmp_train_y = tmp_train_data[feat_to_be_augmented]\n","        \n","        if plot_correlations:\n","            print(\"Plotting y against each X.... \\n\\n \")\n","            for x in high_corr_full_features:\n","                tmp_train_data.plot(x=x, y=feat_to_be_augmented, style='o')\n","                plt.show()\n","\n","        #\"filtering predict-data set where all high-corr-features are not-NA and feat-to-be-augmented are NA\"\n","        query_str_predict = ' & '.join(['~{}.isna()'.format(k) for k in high_corr_full_features])\n","        query_str_predict = ' & '.join([query_str_predict, '{}.isna()'.format(feat_to_be_augmented)])\n","        \n","        \n","        tmp_predict_data  = data_under_cleansing.query(query_str_predict,engine=\"python\")\n","        tmp_predict_X = tmp_predict_data[high_corr_full_features]\n","        lm = LinearRegression().fit(tmp_train_X, tmp_train_y)\n","        model_score =  lm.score(tmp_train_X, tmp_train_y)\n","        print(\"Model score: \", model_score)\n","        if model_score < min_model_score:\n","            # do not use a model to augment data when model is less than 45% accurate. Shifting this threshold to 50% leads to NON-CONVERGENCE\n","            print(\"aborting augmenting..\")\n","            continue\n","\n","        print(\"Model coef: \" , lm.coef_)\n","        tmp_predict_y = list(lm.predict(tmp_predict_X))\n","        feat_column_index = data_under_cleansing.columns.get_loc(feat_to_be_augmented)\n","        index_list = tmp_predict_data.index.tolist()\n","\n","        for i, index in enumerate(index_list):\n","            data_under_cleansing.iloc[index][feat_column_index] = tmp_predict_y[i]\n","\n","    return data_under_cleansing\n","\n","\n","def augment_missing_data(given_dataset, plot_correlations=False, allow_reduced_ref_point=False, min_acceptable_lm_score=0.35):\n","    '''\n","    Utility Method which takes a data set of size `n` with `m`-features and augments \n","    features which are missing using linear-regression models. There is not guarantee that \n","    this augmentation process will converge for all data-sets. But it is known to work for the train-data\n","    from this project.\n","    '''\n","    rogue_columns = ['image', 'Image', 'index', 'Index', 'check_sum']\n","    data_to_be_cleansed = given_dataset[given_dataset.columns.difference(rogue_columns)]\n","    print(data_to_be_cleansed.shape)\n","\n","    incomplete_feature_count = -1\n","\n","    while True:\n","        print(\"\\n\\n==========================================================\")\n","        data_density_mask = get_data_density_mask(data_to_be_cleansed)\n","        incomplete_features = [key for key in data_density_mask.keys() if not data_density_mask[key]]\n","        complete_features    = [key for key in data_density_mask.keys() if data_density_mask[key]]\n","\n","        print(\"Incomplete Features: \", len(incomplete_features))\n","        print(\"Complete Features: \", len(complete_features))\n","\n","        # a feature_threshold to identify how many features are to be used to model \n","        # feature being augmented. Minimum is 2.\n","        reference_point_count = 2\n","\n","        if len(incomplete_features) > 0:\n","            if incomplete_feature_count == len(incomplete_features):\n","              if allow_reduced_ref_point:\n","                if reference_point_count > 1:\n","                  print(\"Model is not converging; reducing reference-point-count to allow convergence.\")\n","                  reference_point_count = 1\n","                else:\n","                  raise Exception(\"Error: Augmentation not converging; Reference-points inadequate; try to SET allow_reduced_ref_point=TRUE if not already\")\n","            else:\n","              incomplete_feature_count = len(incomplete_features)\n","              \n","            data_to_be_cleansed = do_augment_missing_data(data_to_be_cleansed, data_density_mask, plot_correlations, reference_point_count, min_acceptable_lm_score)\n","        else:\n","            break\n","    \n","    return data_to_be_cleansed"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"id":"JA03NbIKo6r3"},"source":["#CODE CELL FOR RAKESH\n","\n","## Pass in the right data-set to augment and get the augmented data back (filtering 'image' column here)\n","augmented_data = augment_missing_data(train_data, allow_reduced_ref_point=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"nteract":{"transient":{"deleting":false}},"id":"sSrERCqQo6r7"},"source":["## Save Augmented Data to a Pickle file"]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"id":"WOB65zZ2o6r7"},"source":["#CODE CELL FOR JACKIE\n","# Pickle train and test so that we can jump in with cleaning this data\n","# pickle.dump( augmented_data, open( \"data/aug_train.p\", \"wb\" ) )\n","import os\n","files = os.listdir(\"CleanTrain\")\n","\n","for filename in files:\n","    print(\"Opening file: \", filename)\n","    clean_file = \"\".join((\"CleanTrain/\",filename))\n","    train_data = pickle.load( open( clean_file, \"rb\" ) )\n","    \n","    print(\"Augmenting Data\")\n","    train_data.rename(columns = {'level_0' : 'index'}, inplace = True)\n","    augmented_data = augment_missing_data(train_data.loc[:, train_data.columns != 'image'])\n","    print(\"Augmented data shape: \", augmented_data.shape)\n","\n","    print(\"Pickle Augmented file\")\n","    aug_file = \"\".join((\"c:/Data/Augmented/aug_\",filename))\n","    #Pickle train and test so that we can jump in with cleaning this data\n","    pickle.dump( augmented_data, open( aug_file, \"wb\" ) )\n","    print()"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Opening file:  clean_all_outliers.p\n","Augmenting Data\n","(7041, 30)\n","\n","\n","==========================================================\n","Incomplete Features:  22\n","Complete Features:  8\n","Model score:  0.5041559390629773\n","Model coef:  [0.6343008  0.33562022]\n"]},{"output_type":"error","ename":"IndexError","evalue":"single positional indexer is out-of-bounds","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-6-44bcce3be53d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Augmenting Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'level_0'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0maugmented_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maugment_missing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Augmented data shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugmented_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-4-b0fb0dec47f3>\u001b[0m in \u001b[0;36maugment_missing_data\u001b[1;34m(given_dataset, plot_correlations, allow_reduced_ref_point, min_acceptable_lm_score)\u001b[0m\n\u001b[0;32m    118\u001b[0m               \u001b[0mincomplete_feature_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincomplete_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mdata_to_be_cleansed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_augment_missing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_be_cleansed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_density_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_correlations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_point_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_acceptable_lm_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-4-b0fb0dec47f3>\u001b[0m in \u001b[0;36mdo_augment_missing_data\u001b[1;34m(data_under_cleansing, density_mask, plot_correlations, reference_point_count, min_model_score)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mdata_under_cleansing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeat_column_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_predict_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_under_cleansing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}