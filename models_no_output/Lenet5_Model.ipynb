{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d230cf8de72e867e5717d0f5cf531d71189c7e5bd77bc2a42cd05122b296a561",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "##  Lenet5 Inspired Models -JackieN \n",
    "This File Produces A number of Lenet5 inspired Models and Predictions based on varying degrees of cleaned Train data.\n",
    "\n",
    "Based on https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086 and \n",
    "\n",
    "https://deepai.org/publication/towards-good-practices-on-building-effective-cnn-baseline-model-for-person-re-identification#:~:text=The%20last%20key%20practice%20is%20to%20train%20CNN,based%20on%20the%20adaptive%20estimates%20of%20lower-order%20moments.\n",
    "\n",
    "The best score produced from the model using the clean data with all outliers removed is: 2.48797  \n",
    "\n",
    "Placing at position 51 on the leaderboard\n",
    "\n",
    "![](https://i.imgur.com/BiDsWBP.jpg)\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Imports\n",
    "\n",
    "Set the UTILS_PATH to be the locaiton of your utils directory.  This will allow for the use of the loading of load_models and predict_models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the utils path to point to the utils directory locally\n",
    "UTILS_PATH = \"MODIFY THIS\"\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(UTILS_PATH)\n",
    "from load_models import LoadTrainModels\n",
    "from predict_models import PredictModels\n",
    "import imp\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Helper Path Functions\n",
    "\n",
    "Two helper functions were created for simplification:\n",
    "\n",
    "- set_train_paths: This sets the following paths and must be done prior to calling create_model() and create_predictions() \n",
    "    - model_path - location the model files should be saved \n",
    "    - train_path - location of the clean train pickle files to use for model creation\n",
    "\n",
    "- set_test_paths: This sets the following paths and must be done prior to calling create_model() and create_predictions() \n",
    "    - test_path - location of test pickle file\n",
    "    - id_lookup - location the id_lookup pickle file \n",
    "    - prediction_path - location of where the prediction csv should be saved\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_train_paths(model_path, train_path=\"C:/Data/CleanTrain/\", ):\n",
    "    global file_path\n",
    "    global trainer\n",
    "    global output_model_path\n",
    "    file_path = train_path\n",
    "    output_model_path=model_path\n",
    "    trainer = LoadTrainModels(output_model_path, file_path)\n",
    "    trainer.print_paths()\n",
    "\n",
    "def set_test_paths(test_path=\"../Data/test.p\", id_lookup_path=\"../Data/id_lookup.p\", prediction_path = \"C:/data/Predictions/\"):\n",
    "    global pred_path\n",
    "    global predictor\n",
    "    global id_lookup\n",
    "    global test\n",
    "\n",
    "    id_lookup = pickle.load( open(id_lookup_path , \"rb\" ) )\n",
    "    test = pickle.load( open(test_path , \"rb\" ) )\n",
    "    pred_path = prediction_path\n",
    "    predictor = PredictModels(output_model_path,pred_path , id_lookup)\n",
    "    predictor.print_paths()\n"
   ]
  },
  {
   "source": [
    "### Helper Model Functions\n",
    "\n",
    "Two helper functions were created for simplification:\n",
    "\n",
    "- create_model: This takes care of opening files in a directory and passing along setting to the utils class LoadTrainModels which will apply any augmentation, split, and train the models. The model files will be stored at the specified location. You must ensure that the set_train_paths() funciton is called prior to this with the appropriate paths set. \n",
    "\n",
    "- create_predictions: This takes care of opening model files in a directory and passing along settings to the utils class PredictModels which will generate a predictions csv per model. You must ensure that the set_train_paths() and set_test_paths() funcitons are called prior to this with the appropriate paths set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(aug = False, vary_layers = False, hoizontal_flip = False,brightness = 1, dim = 1, separate = False):\n",
    "    files = os.listdir(file_path)\n",
    "    num_layers = 6\n",
    "    #For every version of a cleaned Train file in CleanTrain directory, create and save a model\n",
    "    for filename in files: \n",
    "        print(\"Opening file: \", filename)\n",
    "        clean_file = \"\".join((file_path,filename))\n",
    "        train_data = pickle.load( open( clean_file, \"rb\" ) )\n",
    "        train_data = train_data.drop(['level_0', 'check_sum', 'index'], axis=1,errors='ignore')\n",
    "        print(\"Train Shape:\", train_data.shape)\n",
    "        filename = str(filename).replace('.p', '').strip()\n",
    "        \n",
    "\n",
    "        #Setting layers:\n",
    "        #layers = 2 equates to 9 model layers\n",
    "        #layers = 3 equates to 11 model layers\n",
    "        #layers = 4 equates to 13 model layers\n",
    "        #layers >= 5 equates to 15 model layers\n",
    "        if vary_layers:\n",
    "            #Now for each model, let's try different layers\n",
    "            for num_layers in range(2,6):\n",
    "                print(\"Begin model and train:\")\n",
    "                model_name = \"\".join((filename,str(num_layers),\"layers_Lenet5\"))\n",
    "                print(\"Model name:\", model_name)\n",
    "                model, history = trainer.train_model(model_name, train_data, hoizontal_flip = hoizontal_flip,aug = aug, brightness = brightness, dim = dim,layers=num_layers, separate = separate)\n",
    "                print(\"End model and train\")    \n",
    "                print()\n",
    "        else:\n",
    "            print(\"Begin model and train:\")\n",
    "            model_name = \"\".join((filename,\"_Lenet5\"))\n",
    "            print(\"Model name:\", model_name)\n",
    "            model, history = trainer.train_model(model_name, train_data, aug = aug, hoizontal_flip = hoizontal_flip,brightness = brightness, dim = dim,layers=num_layers, separate = separate)\n",
    "            print(\"End model and train\")    \n",
    "            print()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def create_predictions(columns = \"Full\"):\n",
    "    files = os.listdir(output_model_path)\n",
    "    #For every model in file_path, predict using the model and save the predictions in CSV file\n",
    "    for filename in files:\n",
    "        if \".h5\" in filename:\n",
    "            base_name = filename[:-3]\n",
    "            model_json = ''.join((base_name,\".json\"))\n",
    "            print(\"Working with: \", base_name)\n",
    "            print(\"Begin Predict\")\n",
    "            #The predict_standard makes predictions and stores them in a pred_path location speficied.\n",
    "            #pred_path is set via the set_test_paths function call\n",
    "            Y= predictor.predict_standard(base_name, filename, model_json, test, columns=columns)\n",
    "            print(\"End model and train\")    \n",
    "            print()\n",
    "\n",
    "def combine_predictions(full_path, seperate_path):\n",
    "    predictor.combine_predictions(full_path, seperate_path)"
   ]
  },
  {
   "source": [
    "## Baseline test\n",
    "\n",
    "To begin, let's run the model against the raw train data to determine baseline. Once we have a baseline, we can attempt to improve from that.  \n",
    "\n",
    "This cell calls the set_train_paths with the paths of the output of the model creation and the path of the train file. It then creates the model. The directory output is seen below:\n",
    "\n",
    "![](https://i.imgur.com/qT7mF5c.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "set_train_paths(\"C:/data/Jackie_Lenet5_Raw\", \"C:/Data/RawTrain/\")\n",
    "#Get the files in the clean directory\n",
    "create_model()\n",
    "    "
   ]
  },
  {
   "source": [
    "## Baseline Prediction \n",
    "\n",
    "For the model created above, predict using the model and save the predictions in CSV file for submission. \n",
    "\n",
    "Following the cell above, it's now time to make some predictions.  The following cell, \n",
    "\n",
    "- sets the test paths: set the path where train dataset is location, set the path the prediction should be saved\n",
    "- loops through the directory and for each model (json file)\n",
    "    1. Create a prediction and store in specified location.\n",
    "\n",
    "The baseline approach was submitted and recieved the score below: \n",
    "\n",
    "![](https://i.imgur.com/JW2wJfQ.jpg)\n",
    "\n",
    "\n",
    "Note: If you would like to run this cell, please update the paths accordingly. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to make some predictions\n",
    "set_test_paths()\n",
    "create_predictions()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Improvement to Baseline\n",
    "\n",
    "Four attemps were made to improve against baseline and followed the following pipeline approach:\n",
    "\n",
    "![](https://i.imgur.com/jlxPolW.png)\n",
    "\n",
    "- Approach 1: all versions of cleaned train data (clean section) set were used to create models and predictions \n",
    "- Approach 2: all versions of cleaned train data (clean section) set were used + varying the layers in the model used to create models and predictions\n",
    "- Approach 3: all versions of cleaned train data (clean section) set were used + varying layers + image augmentation (brightness and dim) to create models and predictions\n",
    "\n",
    "\n",
    "The following two appoaches take a slightly different approach by creating a model with 30 keypoints and then with 8 keypoints and combining the predictions into a single CSV. \n",
    "\n",
    "![](https://i.imgur.com/1UUDUSy.jpg)\n",
    "\n",
    "- Approach 4: use different cleaned versions of train data set flip the images, add brigthness=0 dim = 0 for 30 keypoints and 8 keypoints. \n",
    "- Approach 5: use only clean_all_outliers of train data set flip the images, add brigthness=1.4 dim = 0.3 for 30 keypoints and 8 keypoints full layers. (best result came from this test)\n",
    "\n",
    "All five are described in the following cells. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Approach 1: Use different cleaned versions of train data set\n",
    "\n",
    "Now it's time to see if we can improve from baseline.  For this attempt, we will create a model for every version of a clean Train file in a given path, create and save a model.\n",
    "\n",
    "![](https://i.imgur.com/S7FhUkH.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This cell looped through the directory of clean trail files which appears below and created a model for each file. Please refer to the Readme file for more information on each.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/bNZTV5a.jpg)\n",
    "\n",
    "\n",
    "\n",
    "The following cell produced the prediction with the best result for the clean file named: clean_all_outliers.  This means that the train file that was cleaned by removing all outliers produced the best result with these settings. \n",
    "\n",
    "![](https://i.imgur.com/kbpD4Eo.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "set_train_paths(model_path= \"C:/data/Jackie_Lenet5_AllClean\")\n",
    "#Get the files in the clean directory and create a model for each\n",
    "create_model(vary_layers = False)\n",
    "\n",
    "#Peform the predictions\n",
    "set_test_paths()\n",
    "create_predictions()"
   ]
  },
  {
   "source": [
    "### Approach 2: Use different cleaned versions of train data set and vary the layers of the model. \n",
    "\n",
    "The following cell is an advanced version.  No transformations to the data were applied but the model is adjusted by adding layers. This cell will create 5 models with varying layers per clean file (e.g. if you have 2 clean files you will end up with 10 models). \n",
    "\n",
    "Setting layers:\n",
    "\n",
    "- layers = 2 equates to 9 model layers\n",
    "- layers = 3 equates to 11 model layers\n",
    "- layers = 4 equates to 13 model layers\n",
    "- layers >= 5 equates to 15 model layers\n",
    "\n",
    "\n",
    "In this example, I only use the clean_all_outliers clean train file since it produced the best results previously.\n",
    "\n",
    "![](https://i.imgur.com/0UPWIj1.jpg)\n",
    "\n",
    "Note: run at your own risk suggest only one clean file in the directory at a time. I did run this on all 7 clean files and it worked! \n",
    "\n",
    "![](https://i.imgur.com/NDedKbW.jpg)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "set_train_paths(model_path= \"C:/data/Jackie_Lenet5_Layers\", train_path= \"C:/Data/CleanTrain_1/\")\n",
    "#Get the files in the clean directory, try different layers to create some models\n",
    "create_model(vary_layers = True)\n",
    "\n",
    "#Peform the predictions\n",
    "set_test_paths()\n",
    "create_predictions()\n",
    "    "
   ]
  },
  {
   "source": [
    "### Approach 3: Use different cleaned versions of train data set and vary the layers of the model and augment the data. \n",
    "\n",
    "The following cell is an advanced version.  The brightness and dim were adjusted on the images and the model is adjusted by adding layers. This cell will creates 4 models with varying layers per clean file (e.g. if you have 2 clean files you will end up with 8 models).  \n",
    "\n",
    "In this example, I only use the clean_all_outliers clean train file since it produced the best results previously.  \n",
    "\n",
    "The best performing was the layers=2 (9 model layers) with bright and dim set but still did not beat approach 1: \n",
    "\n",
    "![](https://i.imgur.com/rF9Crwr.jpg)\n",
    "\n",
    "Note: run at your own risk suggest only one clean file in the directory at a time. I did run this on all 7 clean files and it worked! "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "set_train_paths(model_path = \"C:/data/Jackie_Lenet5_BD\", train_path= \"C:/Data/CleanTrain_1/\")\n",
    "#Get the files in the clean directory, try different layers and ajust the brightness and dim level of each image\n",
    "create_model(vary_layers = True, hoizontal_flip = False,brightness = 1.4, dim = .3)\n",
    "\n",
    "#Peform the predictions\n",
    "set_test_path\n",
    "create_predictions()"
   ]
  },
  {
   "source": [
    "### Approach 4: Use different cleaned versions of train data set flip the images, add brigthness=0 dim = 0 for 30 keypoints and 8 keypoints. \n",
    "\n",
    "The following cell is an advanced version.  The images are flipped horizontaly and brightness and dim are set to 0. \n",
    "\n",
    "For every model file in a given path, \n",
    "\n",
    "- create, train and fit model with augmentation and with 30 keypoints, predict and save predictions\n",
    "- create, train and fit model with augmentation and with 8 keypoints, predict and save predictions\n",
    "- combine the predictions into a single file for submission\n",
    "\n",
    "Output for 30 keypoints: \n",
    "![](https://i.imgur.com/iWpWR5C.jpg)\n",
    "\n",
    "Output for 8 keypoints: \n",
    "![](https://i.imgur.com/I9S2rMh.jpg)\n",
    "\n",
    "\n",
    "In this example, I only use the clean_all_outliers clean train file since it produced the best results previously.  \n",
    "\n",
    "This yielded the 3rd best result:  \n",
    "\n",
    "\n",
    "![](https://i.imgur.com/RrpnwyW.jpg)\n",
    "\n",
    "\n",
    "Note: one less layer was tested here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Use 30 keypoints\n",
    "full_path = \"C:/data/Jackie_Lenet5_30RawAug\"\n",
    "\n",
    "set_train_paths(full_path, \"C:/Data/CleanTrain_30/\")\n",
    "#Get the files in the clean directory, try different layers to create some models and flip the images horizontally\n",
    "#create_model(vary_layers = False, hoizontal_flip = False,brightness = 1, dim = 1, seperate = False)\n",
    "create_model((aug = True, hoizontal_flip = True, brightness = 0, dim = 0)\n",
    "\n",
    "#Peform the predictions\n",
    "set_test_paths(prediction_path = \"C:/data/Predictions_30/\")\n",
    "create_predictions(columns = \"Full\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only 8 keypoints\n",
    "seperate_path = \"C:/data/Jackie_Lenet5_8RawAug\"\n",
    "set_train_paths(seperate_path, \"C:/Data/CleanTrain_8/\")\n",
    "create_model((aug = True, hoizontal_flip = True, brightness = 0, dim = 0, separate = True)\n",
    "set_test_paths(prediction_path = \"C:/data/Predictions_8/\")\n",
    "create_predictions(columns = \"False\")\n",
    "\n",
    "combine_predictions(\"C:/data/Predictions_30/\", \"C:/data/Predictions_8/\" )"
   ]
  },
  {
   "source": [
    "### Approach 5: Use only clean_all_outliers of train data set flip the images, add brigthness=1.4 dim = 0.3 for 30 keypoints and 8 keypoints - full layers. \n",
    "\n",
    "The following cell is an advanced version.  The images are flipped horizontaly and brightness=1.4 and dim=0.3 \n",
    "\n",
    "For every model file in a given path, \n",
    "\n",
    "- create, train and fit model with augmentation and with 30 keypoints, predict and save predictions\n",
    "- create, train and fit model with augmentation and with 8 keypoints, predict and save predictions\n",
    "- combine the predictions into a single file for submission\n",
    "\n",
    "Output for 30 keypoints:\n",
    "\n",
    "![](https://i.imgur.com/yWrWgCB.jpg)\n",
    "\n",
    "Output for 8 keypoints: \n",
    "\n",
    "![](https://i.imgur.com/y6iDWSc.jpg)\n",
    "\n",
    "In this example, I only use the clean_all_outliers clean train file since it produced the best results previously. \n",
    "\n",
    "![](https://i.imgur.com/YuhHwCL.jpg)\n",
    "\n",
    "This yielded the best result:  \n",
    "\n",
    "![](https://i.imgur.com/BiDsWBP.jpg)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 30 keypoints\n",
    "full_path = \"C:/data/Jackie_Lenet5_30_1_layer\"\n",
    "\n",
    "set_train_paths(full_path, \"C:/Data/CleanTrain_30_1/\")\n",
    "#Get the files in the clean directory, try different layers to create some models and flip the images horizontally\n",
    "#create_model(vary_layers = False, hoizontal_flip = False,brightness = 1, dim = 1, seperate = False)\n",
    "create_model(aug = True, hoizontal_flip = True, brightness = 1.4, dim = 0.3)\n",
    "\n",
    "#Peform the predictions\n",
    "set_test_paths(prediction_path = \"C:/data/Predictions_30_1_layer/\")\n",
    "create_predictions(columns = \"Full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use only 8 keypoints\n",
    "seperate_path = \"C:/data/Jackie_Lenet5_8_1_layer\"\n",
    "set_train_paths(seperate_path, \"C:/Data/CleanTrain_8_1/\")\n",
    "create_model(aug = True,hoizontal_flip = True, brightness = 0, dim = 0, separate = True)\n",
    "set_test_paths(prediction_path = \"C:/data/Predictions_8_1/\")\n",
    "create_predictions(columns = \"False\")\n",
    "\n",
    "\n",
    "combine_predictions(\"C:/data/Predictions_30_1/\", \"C:/data/Predictions_8_1_layer/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}