{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from tqdm import tqdm\n",
        "import zlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib import rc\n",
        "from matplotlib.ticker import PercentFormatter"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1614473345343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.show_versions()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614473347869
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data For EDA\n",
        "There are 4 files found in the data directory:\n",
        "\n",
        "- IdLookupTable - TBD\n",
        "- SampleSubmission - TBD\n",
        "- test - TBD\n",
        "- training - TBD \n",
        "\n",
        "We must load this data in order to perform EDA. \n",
        "\n",
        "TBD ADD MORE DETAILS HERE ABOUT GENERAL EDA APPRAOCH. \n",
        "\n",
        "EDA\n",
        "1. DUPLICATES\n",
        "2. OUTLIERS\n",
        "3. MISSING DATA\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#https://realpython.com/python-zip-function/#:~:text=%20Using%20the%20Python%20zip%20()%20Function%20for,zip%20()%20function%20works%20differently%20in...%20More\n",
        "\n",
        "df, git_path = {}, 'data/'\n",
        "for file_name, file_ref, n, t in zip(['test.csv', 'training.csv', 'IdLookupTable.csv', 'SampleSubmission.csv'],\n",
        "                        ['test', 'train', 'id_lookup', 'sample_submission', ],\n",
        "                        [   #test\n",
        "                            ['image_id', 'image'], \n",
        "                            #train\n",
        "                            ['left_eye_center_x', 'left_eye_center_y',  \n",
        "                            'right_eye_center_x', 'right_eye_center_y', \n",
        "                            'left_eye_inner_corner_x', 'left_eye_inner_corner_y', \n",
        "                            'left_eye_outer_corner_x', 'left_eye_outer_corner_y', \n",
        "                            'right_eye_inner_corner_x', 'right_eye_inner_corner_y', \n",
        "                            'right_eye_outer_corner_x', 'right_eye_outer_corner_y', \n",
        "                            'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y', \n",
        "                            'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y', \n",
        "                            'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', \n",
        "                            'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y', \n",
        "                            'nose_tip_x', 'nose_tip_y', \n",
        "                            'mouth_left_corner_x', 'mouth_left_corner_y', \n",
        "                            'mouth_right_corner_x', 'mouth_right_corner_y', \n",
        "                            'mouth_center_top_lip_x', 'mouth_center_top_lip_y', \n",
        "                            'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'image'],\n",
        "                            #IdLookupTable\n",
        "                            ['row_id', 'image_id', 'feature_name', 'location'],\n",
        "                            #SampleSubmission\n",
        "                            ['row_id', 'location']\n",
        "                        ],\n",
        "\n",
        "                        [\n",
        "                             #test   \n",
        "                            {'image_id':'uint16', 'image':'object'},\n",
        "                            #train\n",
        "                            {'left_eye_center_x':'float32', 'left_eye_center_y':'float32', \n",
        "                            'right_eye_center_x':'float32', 'right_eye_center_y':'float32', \n",
        "                            'left_eye_inner_corner_x':'float32', 'left_eye_inner_corner_y':'float32', \n",
        "                            'left_eye_outer_corner_x':'float32', 'left_eye_outer_corner_y':'float32', \n",
        "                            'right_eye_inner_corner_x':'float32', 'right_eye_inner_corner_y':'float32',\n",
        "                            'right_eye_outer_corner_x':'float32', 'right_eye_outer_corner_y':'float32', \n",
        "                            'left_eyebrow_inner_end_x':'float32', 'left_eyebrow_inner_end_y':'float32',\n",
        "                            'left_eyebrow_outer_end_x':'float32', 'left_eyebrow_outer_end_y':'float32', \n",
        "                            'right_eyebrow_inner_end_x':'float32', 'right_eyebrow_inner_end_y':'float32',\n",
        "                            'right_eyebrow_outer_end_x':'float32', 'right_eyebrow_outer_end_y':'float32', \n",
        "                            'nose_tip_x':'float32', 'nose_tip_y':'float32', 'mouth_left_corner_x':'float32',\n",
        "                            'mouth_left_corner_y':'float32', 'mouth_right_corner_x':'float32', \n",
        "                            'mouth_right_corner_y':'float32', 'mouth_center_top_lip_x':'float32', \n",
        "                            'mouth_center_top_lip_y':'float32','mouth_center_bottom_lip_x':'float32', \n",
        "                            'mouth_center_bottom_lip_y':'float32', 'image':'object'},\n",
        "                             #IdLookupTable\n",
        "                            {'row_id':'uint16', 'image_id':'uint16', 'location':'float32'},\n",
        "                            #SampleSubmission\n",
        "                            {'row_id':'uint16', 'location':'float32'}\n",
        "                        ],\n",
        "                        ):\n",
        "    #This is the begining of the for loop for each file:\n",
        "    print(\"Load files.\")\n",
        "    print(\"Begin loading file '%s' \" % \"\".join( (git_path, file_name)))\n",
        "    #print(file_ref)\n",
        "    df[file_ref] = pd.read_csv(\"\".join( (git_path,file_name) ), names = n, dtype = t, skiprows = 1)\n",
        "    \n",
        "    #If the file contains an image column like in the case of test.csv store those images now. \n",
        "    if \"image\" in df[file_ref]:\n",
        "        print(\"\\tFound %d images. Processing. \" % df[file_ref].shape[0])\n",
        "        #Get the row with the image data and store it in the dataframe \n",
        "        df[file_ref]['image'] = df[file_ref][\"image\"].map(lambda x: np.array(list(map(int, x.split(\" \")))))\n",
        "    print(\"\\tFile\", file_ref, \" with shape:\", df[file_ref].shape, \" load complete\\n\")\n",
        "\n",
        "print(\"Load files complete\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473383715
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df['train'][['image']], df['test'][['image']]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473624022
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HELPER FUNCTIONS to reset the train and test dataframes\n",
        "\n",
        "def reset_train_df():\n",
        "    train = df['train'].reset_index().copy()\n",
        "    #Get the images and perform a checksum on every image in train: https://www.geeksforgeeks.org/zlib-adler32-in-python/\n",
        "    train['check_sum'] = train.image.map(lambda x: zlib.adler32(x))\n",
        "\n",
        "    return train\n",
        "\n",
        "def reset_test_df():\n",
        "    test = df['test'].reset_index().copy()\n",
        "    #Get the images and perform a checksum on every image in train: https://www.geeksforgeeks.org/zlib-adler32-in-python/\n",
        "    test['check_sum'] = test.image.map(lambda x: zlib.adler32(x))\n",
        "\n",
        "    return test\n",
        "\n",
        "def get_coordinate_columns():\n",
        "    coordinates = [c for c in train.columns if c.endswith('_x') | c.endswith('_y')]\n",
        "    return coordinates"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate train images\n",
        "\n",
        "#Get the reset train df\n",
        "train = reset_train_df()\n",
        "#Create a DF to store duplicates, grouping them together and sorting them\n",
        "train_duplicates = pd.DataFrame(train.groupby(by='check_sum').index.count().sort_values()).reset_index()\n",
        "#Add a column to keep track of how many of each check sum there are\n",
        "train_duplicates.columns = ['check_sum', 'number_found']\n",
        "#Keep the ones where we have > 1 number_found\n",
        "train_duplicates = train_duplicates[(train_duplicates.number_found > 1)]\n",
        "#Now do a left outer join back to train_duplicates.  This should only keep the duplicates \n",
        "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#:~:text=merge%20is%20a%20function%20in%20the%20pandas%20namespace,,the%20index-on-index%20(by%20default)%20and%20column%20(s)-on-index%20join.\n",
        "train_duplicates = pd.merge(train_duplicates, train[['index', 'check_sum']],  how = 'left', on=['check_sum']).sort_values(by=['number_found', 'check_sum'], ascending = False)\n",
        "\n",
        "\n",
        "#Now do the same for test:\n",
        "# Check for duplicate train images\n",
        "test = reset_test_df()\n",
        "#Create a DF to store duplicates, grouping them together and sorting them\n",
        "test_duplicates = pd.DataFrame(test.groupby(by='check_sum').index.count().sort_values()).reset_index()\n",
        "#Add a column to keep track of how many of each check sum there are\n",
        "test_duplicates.columns = ['check_sum', 'number_found']\n",
        "#Keep the ones where we have > 1 number_found\n",
        "test_duplicates = test_duplicates[(test_duplicates.number_found > 1)]\n",
        "#Now do a left outer join back to train_duplicates.  This should only keep the duplicates \n",
        "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#:~:text=merge%20is%20a%20function%20in%20the%20pandas%20namespace,,the%20index-on-index%20(by%20default)%20and%20column%20(s)-on-index%20join.\n",
        "test_duplicates = pd.merge(test_duplicates, test[['index', 'check_sum']],  how = 'left', on=['check_sum']).sort_values(by=['number_found', 'check_sum'], ascending = False)\n",
        "\n",
        "print(\"EDA on duplicate data in train and test datasets: \")\n",
        "print(\"The train dataset has %d unique images out of the %d duplicate images from the total of %d images\" % (len(np.unique(train_duplicates.check_sum)), len(train_duplicates), train.size))\n",
        "print(\"The test dataset has %d unique images out of %d duplicate images from the total of %d images\" % (len(np.unique(test_duplicates.check_sum)),len(test_duplicates), test.size))\n",
        "\n",
        "#Clean up:\n",
        "#We don't really need the check_sum column anymore...so drop it\n",
        "train.drop(columns=['check_sum'], inplace=True)\n",
        "test.drop(columns=['check_sum'], inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473627155
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.size)\n",
        "print(train_duplicates.size)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473630874
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## TRAIN \n",
        "# Let's view some of these duplicated train images\n",
        "fig = plt.figure(figsize=(18,18))\n",
        "fig.suptitle('Sample of duplicate images from the Train dataset\\n n= 35', size = 20,  y = 1.04, weight = 'bold')\n",
        "#Get the point coordinates for example: mouth_center_top_lip_x\n",
        "coordinates = get_coordinate_columns()\n",
        "#print(coordinates)\n",
        "#Get the top 35 duplicate images\n",
        "idx = train_duplicates.head(35)['index'].values\n",
        "#For testing, these are the duplicate ID's\n",
        "print(idx)\n",
        "\n",
        "match_pts = pd.DataFrame(columns =['Points_Found', 'Count'])\n",
        "\n",
        "#Loop through and plot each of the 35 images.  \n",
        "for i, idx in enumerate(idx):\n",
        "    plt.subplot(7,5,i+1)\n",
        "    img = train[(train['index'] == idx)].image.values[0].reshape(96,96)\n",
        "    #These are the points that have been identified on the images\n",
        "    points = train[(train['index'] == idx)][coordinates].values[0]\n",
        "    plt.imshow(img, cmap = 'gray')\n",
        "    plt.axis('off')\n",
        "    matching_pts = 0\n",
        "\n",
        "    for pts in range(0, 30, 2):\n",
        "        x_point, y_point = (points[pts], points[pts+1])\n",
        "        if not (np.isnan(x_point)) and not (np.isnan(y_point)):\n",
        "            matching_pts += 1\n",
        "            #Add the point to the plot\n",
        "            plt.plot(x_point, y_point, 'o', color = \"red\", markersize = 5)\n",
        "\n",
        "    plt.title(\"Image #:[%d]\\n#Points:[%d]\" % (idx, matching_pts))\n",
        "    if matching_pts in match_pts[\"Points_Found\"].values:\n",
        "            match_pts.loc[match_pts['Points_Found'] == matching_pts, 'Count'] = match_pts.loc[match_pts['Points_Found'] == matching_pts, 'Count'] + 1\n",
        "    else:\n",
        "        match_pts = match_pts.append({'Points_Found':matching_pts,'Count': 1},ignore_index=True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473635241
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = train_duplicates['index'].values\n",
        "#For testing, these are the duplicate ID's\n",
        "print(idx)\n",
        "match_pts = pd.DataFrame(columns =['Points_Found', 'Count'])\n",
        "coordinates = get_coordinate_columns()\n",
        "#Loop through and plot each of the 35 images.  \n",
        "for i, idx in enumerate(idx):\n",
        "    img = train[(train['index'] == idx)].image.values[0].reshape(96,96)\n",
        "    #These are the points that have been identified on the images\n",
        "    points = train[(train['index'] == idx)][coordinates].values[0]\n",
        "    matching_pts = 0\n",
        "\n",
        "    for pts in range(0, 30, 2):\n",
        "        x_point, y_point = (points[pts], points[pts+1])\n",
        "        if not (np.isnan(x_point)) and not (np.isnan(y_point)):\n",
        "            matching_pts += 1\n",
        "            \n",
        "    if matching_pts in match_pts[\"Points_Found\"].values:\n",
        "            match_pts.loc[match_pts['Points_Found'] == matching_pts, 'Count'] = match_pts.loc[match_pts['Points_Found'] == matching_pts, 'Count'] + 1\n",
        "            \n",
        "    else:\n",
        "        match_pts = match_pts.append({'Points_Found': matching_pts,'Count': 1},ignore_index=True)\n",
        "        \n",
        "print(match_pts)\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "ax.set_title('Number of Points Found on Train Duplicate Data', fontsize = 20, fontweight = 'bold')\n",
        "ax.bar(match_pts.Points_Found, match_pts.Count, width = 1.7)\n",
        "ax.set_xticks(range(0,18,1))\n",
        "for i, r in match_pts.iterrows():\n",
        "    plt.text(r.Points_Found, r.Count + 25, format(r.Count, \",d\"), \n",
        "        horizontalalignment = 'center', verticalalignment = 'center', fontweight ='bold')\n",
        "ax.spines[\"top\"].set_alpha(.0)\n",
        "ax.spines[\"bottom\"].set_alpha(.3)\n",
        "ax.spines[\"right\"].set_alpha(.0)\n",
        "ax.spines[\"left\"].set_alpha(.0)\n",
        "ax.set_xlabel(\"Number Of Points Found On Image\", fontsize = 12, horizontalalignment='center')\n",
        "ax.set_ylabel(\"Number of Duplicate Train Images\", fontsize = 12, horizontalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1614473641847
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_duplicates.columns)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473646017
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do the duplicate Train images have the same labels? Let's test one out. \n",
        "\n",
        "#Get only the first images checksum from duplicate train and then get the images that match the check_sum\n",
        "duplicate_image_chksum = train_duplicates.iloc[0, train_duplicates.columns.get_loc('check_sum')] \n",
        "\n",
        "duplicate_image_index = train_duplicates.loc[(train_duplicates.check_sum == duplicate_image_chksum)]['index'].values\n",
        "\n",
        "#Create an array of all of the coumns with x,y in them\n",
        "coordinate_columns = get_coordinate_columns()\n",
        "\n",
        "#Get the df so we can display something meaningful\n",
        "duplicate_image_df = train.loc[(train['index'].isin(duplicate_image_index))][coordinate_columns]\n",
        "\n",
        "#https://mode.com/example-gallery/python_dataframe_styling/\n",
        "duplicate_image_df.style\\\n",
        "    .highlight_max(subset=coordinate_columns,color='green')\\\n",
        "    .set_na_rep(\"N/A\").format(None, na_rep=\"Missing\")\\\n",
        "    .highlight_null('yellow')\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473685710
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The lables do not match exactly in the duplicate Train images.  The challenge would be to determine which of the images to keep if we remove all but one of the duplicate images. We have two options:\n",
        "\n",
        "#1 - Keep the first duplicate and disregard the others - Easy to do, low cost but we risk losing data.\n",
        "#2 - Take the average for all coordiantes across the duplicate image and apply those coordinates moving forward. A little more work invovled and risk of introducing more errors to the lables. \n",
        "\n",
        "#If we were to do #2 this is how the above image would reconcile:\n",
        "#Take the mean of the columns and create a new DF\n",
        "duplicate_image_df = pd.DataFrame(train.loc[(train['index'].isin(duplicate_image_index))][coordinate_columns].mean())\n",
        "\n",
        "#Display results\n",
        "duplicate_image_df.T.style\\\n",
        "    .set_na_rep(\"N/A\").format(None, na_rep=\"Missing\")\\\n",
        "    .highlight_null('yellow')\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473696107
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Dataset Duplicate Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test = reset_test_df()\n",
        "print(test.size)\n",
        "print(test_duplicates.size)\n",
        "print(test.size/test_duplicates.size, \"% of test data is duplicates\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473702097
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TEST \n",
        "# Let's view some of these duplicated train images\n",
        "fig = plt.figure(figsize=(18,18))\n",
        "fig.suptitle('Sample of duplicate images from the Test dataset\\n n= 35', size = 20,  y = 1.04, weight = 'bold')\n",
        "\n",
        "\n",
        "#Get the top 35 duplicate images\n",
        "idx = test_duplicates.head(35)['index'].values\n",
        "#For testing, these are the duplicate ID's\n",
        "print(idx)\n",
        "\n",
        "#Loop through and plot each of the 35 images.  \n",
        "for i, idx in enumerate(idx):\n",
        "    plt.subplot(7,5,i+1)\n",
        "    img = test[(test['index'] == idx)].image.values[0].reshape(96,96)\n",
        "    plt.imshow(img, cmap = 'gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Image #:[%d]\" % (idx))\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1614473706915
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplication Conclusions -EDA on duplicate data in train and test datasets:\n",
        "Train:\n",
        "\n",
        "1. The train dataset has 543 unique images out of the 1098 duplicate images from the total of 232617 images\n",
        "\n",
        "2. Of the 1098 duplicate images:\n",
        "    - 1096 of them had 4 points\n",
        "    - 1 had  13 points\n",
        "    - 1 had 15 points\n",
        "\n",
        "Test:\n",
        "\n",
        "1. The test dataset has 29 unique images out of 60 duplicate images from the total of 7132 images\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR JACKIE\n",
        "# Remove duplicates in the train dataset by taking the mean of all values for that image in each label \n",
        "def remove_train_duplicates(verbose=True):\n",
        "    # First let's reset the index since we've been working on the df \n",
        "    train = reset_train_df()\n",
        "    train_duplicates.reset_index()\n",
        "\n",
        "    #Get all of the coordinates\n",
        "    coordinates = get_coordinate_columns()\n",
        "\n",
        "    #Create an empty df with the coordinate columns in place\n",
        "    final_images = train[(train.index == -1)][coordinates].copy()\n",
        "\n",
        "    #For each unique check_sum in duplicates...\n",
        "    for check_sum in train_duplicates.check_sum.unique():\n",
        "        #Get all of the duplicates with the same check_sum\n",
        "        duplicates = train_duplicates[(train_duplicates.check_sum == check_sum)]['index'].values\n",
        "        #Get the first image that appears in the train dataset with this check_sum\n",
        "        image = train[(train['index'].isin(duplicates))].image.values[0]\n",
        "        #Take the mean of all the coordinate columns - this is what we will use for the final single image\n",
        "        fixed = pd.DataFrame(pd.DataFrame(train[(train['index'].isin(duplicates))], columns=coordinates).mean(axis = 0)).T\n",
        "        #Make sure to include the actual image (lol)\n",
        "        fixed['image'] = [image]\n",
        "        #Append it to the list of final_images\n",
        "        final_images = final_images.append(fixed, ignore_index = True)\n",
        "        \n",
        "    #For reporting purposes: \n",
        "    if verbose: print(\"Applying EDA fix for duplicates\")\n",
        "    if verbose: print(\"=\"*13 + \"Train\" + \"=\"*13)\n",
        "    if verbose: print(\"Before delete:     %s\" % str(train.shape))\n",
        "\n",
        "    #Remove the duplicates from train - danger, danger, must replace them\n",
        "    train = train[~(train['index'].isin(train_duplicates['index'].values))]\n",
        "    if verbose: print(\"After  delete:     %s\" % str(train.shape))\n",
        "\n",
        "    #Replace removed duplicates with final_images\n",
        "    train = train.append(final_images, ignore_index = True).reset_index()\n",
        "    train.drop(columns=['index'], inplace = True)\n",
        "    if verbose: print(\"After  append:     %s\" % str(train.shape))\n",
        "    return train\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR JACKIE\n",
        "##########Test Data set\n",
        "\n",
        "#Now do the same for test, this will be easier since we don't need\n",
        "#to deal with points and taking the mean\n",
        "def remove_test_duplicates(verbose=True):\n",
        "#We can do this differently since we don't need to take the mean. \n",
        "#Go through the test and only add items to the final test image if\n",
        "#we do not already have the check_sum. If we find the check_sum, don't\n",
        "#add it it's a duplicate. \n",
        "    test = reset_test_df()\n",
        "    if verbose: print(\"=\"*13 + \"Test=\" + \"=\"*13)\n",
        "    if verbose: print(\"Before delete:     %s\" % str(test.shape))\n",
        "    test = reset_test_df()\n",
        "    #Create an empty df with the coordinate columns in place\n",
        "    final_test_images = test[(test.index == -1)]\n",
        "    \n",
        "    for test_index, check_sum in zip(test['index'], test.check_sum):\n",
        "        if not (check_sum in list(final_test_images.check_sum.values)):\n",
        "            final_test_images = final_test_images.append(test.loc[(test['index'] == test_index)], ignore_index = True)\n",
        "    \n",
        "    if verbose: print(\"After  delete:     %s\" % str(test.shape))\n",
        "    return final_test_images\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR JACKIE\n",
        "train = remove_train_duplicates()\n",
        "print()\n",
        "test = remove_test_duplicates()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CELL FOR JACKIE"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR JOANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR RAKESH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CODE CELL FOR SANDIP"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.9.1-final",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}