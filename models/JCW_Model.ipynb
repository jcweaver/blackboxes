{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layered Model & Explorations - JoanieW\n",
    "\n",
    "Based on: https://elix-tech.github.io/ja/2016/06/02/kaggle-facial-keypoints-ja.html#conv\n",
    "\n",
    "And modeled after Jackie's code structure for Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imp\n",
    "import pickle\n",
    "from utils import predict_models, load_models, transform_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joanieweaver/Desktop/intro_ml/blackboxes\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: data/models/\n",
      "Pickle dir: cleantrain/\n"
     ]
    }
   ],
   "source": [
    "### this is gold right here.\n",
    "imp.reload(load_models)\n",
    "file_path = \"cleantrain/\"\n",
    "trainer = load_models.LoadTrainModels(\"data/models/\", file_path)\n",
    "\n",
    "trainer.print_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every version of a clean Train file in a given path, create and save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file:  clean_w_dups.p\n",
      "Train Shape: (6488, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_w_dups_jcw\n",
      "Scaling 6488 images...\n",
      "Scaling of 6488 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_w_dups_jcw\n",
      "End model and train\n",
      "\n",
      "Opening file:  clean_w_outliers.p\n",
      "Train Shape: (7041, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_w_outliers_jcw\n",
      "Scaling 7041 images...\n",
      "Scaling of 7041 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_w_outliers_jcw\n",
      "End model and train\n",
      "\n",
      "Opening file:  clean_o_dups.p\n",
      "Train Shape: (6483, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_o_dups_jcw\n",
      "Scaling 6483 images...\n",
      "Scaling of 6483 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_o_dups_jcw\n",
      "End model and train\n",
      "\n",
      "Opening file:  clean_wo_dups.p\n",
      "Train Shape: (6478, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_wo_dups_jcw\n",
      "Scaling 6478 images...\n",
      "Scaling of 6478 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_wo_dups_jcw\n",
      "End model and train\n",
      "\n",
      "Opening file:  clean_all_outliers.p\n",
      "Train Shape: (7041, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_all_outliers_jcw\n",
      "Scaling 7041 images...\n",
      "Scaling of 7041 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_all_outliers_jcw\n",
      "End model and train\n",
      "\n",
      "Opening file:  clean_duplicates.p\n",
      "Train Shape: (6494, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_duplicates_jcw\n",
      "Scaling 6494 images...\n",
      "Scaling of 6494 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_duplicates_jcw\n",
      "End model and train\n",
      "\n",
      "Opening file:  clean_o_outliers.p\n",
      "Train Shape: (7020, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_o_outliers_jcw\n",
      "Scaling 7020 images...\n",
      "Scaling of 7020 observations complete.\n",
      "Begining the split of Train with all features\n",
      "Looking for model JW\n",
      "Loading model: clean_o_outliers_jcw\n",
      "End model and train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imp.reload(load_models)\n",
    "\n",
    "file_path = \"cleantrain/\"\n",
    "#files = os.listdir(\"../CleanTrain\")\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "#For every version of a cleaned Train file in CleanTrain directory, create and save a model\n",
    "for filename in files: \n",
    "    print(\"Opening file: \", filename)\n",
    "    clean_file = \"\".join((file_path,filename))\n",
    "    train_data = pickle.load( open( clean_file, \"rb\" ) )\n",
    "    train_data = train_data.drop(['level_0', 'check_sum', 'index'], axis=1,errors='ignore')\n",
    "    print(\"Train Shape:\", train_data.shape)\n",
    "\n",
    "    filename = str(filename).replace('.p', '').strip()\n",
    "    print(\"Begin model and train:\")\n",
    "    model_name = \"\".join((filename,\"_jcw\"))\n",
    "    print(\"Model name:\", model_name)\n",
    "    model, history = trainer.train_model(model_name, train_data,verbose = True)\n",
    "    print(\"End model and train\")    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every model file in a given path, predict using the model and save the predictions in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: data/models/\n",
      "Pickle dir: data/predictions/\n",
      "Working with:  clean_test_jcw\n",
      "Begin Predict\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PredictModels' object has no attribute 'predict_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b83ceb89d405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working with: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Begin Predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End model and train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PredictModels' object has no attribute 'predict_model'"
     ]
    }
   ],
   "source": [
    "imp.reload(predict_models)\n",
    "imp.reload(transform_data)\n",
    "\n",
    "id_lookup = pickle.load( open( \"data/id_lookup.p\", \"rb\" ) )\n",
    "test = pickle.load( open( \"data/test.p\", \"rb\" ) )\n",
    "\n",
    "#Using local paths as this is way faster...\n",
    "file_path = \"data/models/\"\n",
    "pred_path = \"data/predictions/\"\n",
    "\n",
    "predictor = predict_models.PredictModels(file_path,pred_path , id_lookup)\n",
    "\n",
    "predictor.print_paths()\n",
    "\n",
    "files = os.listdir(file_path)\n",
    "#For every model in file_path, predict using the model and save the predictions in CSV file\n",
    "for filename in files:\n",
    "    if \".h5\" in filename:\n",
    "        base_name = filename[:-3]\n",
    "        model_json = ''.join((base_name,\".json\"))\n",
    "        print(\"Working with: \", base_name)\n",
    "        print(\"Begin Predict\")\n",
    "        Y= predictor.predict_model(base_name, filename, model_json, test)\n",
    "        print(\"End model and train\")    \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Sandip's models\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance = metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error = metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse = metrics.mean_squared_error(y_true, y_pred) \n",
    "    #mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error = metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    #print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
