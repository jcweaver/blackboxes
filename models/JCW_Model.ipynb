{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layered Model & Explorations - JoanieW\n",
    "\n",
    "Based on: https://elix-tech.github.io/ja/2016/06/02/kaggle-facial-keypoints-ja.html#conv\n",
    "\n",
    "And modeled after Jackie's code structure for Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imp\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import predict_models, load_models, transform_data\n",
    "from keras.models import Sequential, Model, model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is gold right here.\n",
    "imp.reload(load_models)\n",
    "file_path = \"cleantrain/\"\n",
    "trainer = load_models.LoadTrainModels(\"data/models/\", file_path)\n",
    "\n",
    "trainer.print_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every version of a clean Train file in a given path, create and save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp.reload(load_models)\n",
    "\n",
    "file_path = \"cleantrain/\"\n",
    "#files = os.listdir(\"../CleanTrain\")\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "#For every version of a cleaned Train file in CleanTrain directory, create and save a model\n",
    "for filename in files: \n",
    "    print(\"Opening file: \", filename)\n",
    "    clean_file = \"\".join((file_path,filename))\n",
    "    print(clean_file)\n",
    "    train_data = pickle.load( open( clean_file, \"rb\" ) )\n",
    "    train_data = train_data.drop(['level_0', 'check_sum', 'index'], axis=1,errors='ignore')\n",
    "    print(\"Train Shape:\", train_data.shape)\n",
    "\n",
    "    filename = str(filename).replace('.p', '').strip()\n",
    "    print(\"Begin model and train:\")\n",
    "    model_name = \"\".join((filename,\"_jcw\"))\n",
    "    print(\"Model name:\", model_name)\n",
    "    #Run with separate = True to train on 8 columns of data\n",
    "    model, history = trainer.train_jcw(model_name, train_data,verbose = True, separate = True)\n",
    "    print(\"End model and train\")    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every model file in a given path, predict using the model and save the predictions in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(predict_models)\n",
    "imp.reload(transform_data)\n",
    "\n",
    "id_lookup = pickle.load( open( \"data/id_lookup.p\", \"rb\" ) )\n",
    "test = pickle.load( open( \"data/test.p\", \"rb\" ) )\n",
    "\n",
    "#Using local paths as this is way faster...\n",
    "file_path = \"data/models/\"\n",
    "pred_path = \"data/predictions/\"\n",
    "\n",
    "predictor = predict_models.PredictModels(file_path,pred_path , id_lookup)\n",
    "\n",
    "predictor.print_paths()\n",
    "\n",
    "files = os.listdir(file_path)\n",
    "#For every model in file_path, predict using the model and save the predictions in CSV file\n",
    "for filename in files:\n",
    "    if \".h5\" in filename:\n",
    "        base_name = filename[:-3]\n",
    "        model_json = ''.join((base_name,\".json\"))\n",
    "        print(\"Working with: \", base_name)\n",
    "        print(\"Begin Predict\")\n",
    "        #Set columns = False if only predicting 8 keypoints\n",
    "        Y= predictor.predict_jcw(base_name, filename, model_json, test, columns=False)\n",
    "        print(\"End model and train\")    \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations on using different models for 8 vs. 30 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(predict_models)\n",
    "imp.reload(transform_data)\n",
    "\n",
    "id_lookup = pickle.load( open( \"data/id_lookup.p\", \"rb\" ) )\n",
    "test = pickle.load( open( \"data/test.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering to ids where image_id count == 8\n",
    "new_id = id_lookup.groupby(\"image_id\").agg('count')\n",
    "new_id_2 = new_id[new_id[ \"row_id\" ] <= 8]\n",
    "new_id_1 = new_id[new_id[ \"row_id\" ] > 8]\n",
    "print(new_id_2.index)\n",
    "print(new_id_1.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lookup_1 = id_lookup[id_lookup['image_id']<=591]\n",
    "id_lookup_2 = id_lookup[id_lookup['image_id'] > 591]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = test[test['image_id']<=591]\n",
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test[test['image_id']>591]\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_2,open(\"data/test_8keypoints.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify where in the id_lookup is the last output row\n",
    "print(np.max(id_lookup_1['row_id']))\n",
    "print(np.min(id_lookup_1['row_id']))\n",
    "\n",
    "max_row_full = np.max(id_lookup_1['row_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 591 rows of the id lookups require more than 8 keypoints. Afterwards the next 1192 rows only require 6 or 8 keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lookup = id_lookup_2\n",
    "test = test_2\n",
    "pred_path = \"data/predictions/\"\n",
    "\n",
    "predictor = predict_models.PredictModels(file_path,pred_path , id_lookup)\n",
    "\n",
    "model_json = \"data/models/clean_w_outliers_jcw.json\"\n",
    "model_file = \"data/models/clean_w_outliers_jcw.h5\"\n",
    "json_file = open(model_json, \"r\")\n",
    "model_json_data = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json_data)\n",
    "model.load_weights(model_file)\n",
    "base_name = \"clean_w_outliers_jcw\"\n",
    "filename = model_file\n",
    "Y= predictor.predict_jcw(base_name, filename, model_json, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd intro_ml/blackboxes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in prediction csvs to combine to make final prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/predictions/\"\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "for file in files:\n",
    "    #separate is from the most recent run of predictions based solely on train data that had 8 keypoints\n",
    "    predictions_separate= pd.read_csv(\"data/predictions/\"+file)\n",
    "    #full is the older predictions data that was built on all the train\n",
    "    predictions_full = pd.read_csv(\"data/predictions_full/\"+file)\n",
    "    new_pred = predictions_separate[predictions_separate['RowId']>max_row_full].copy()\n",
    "    new_pred = new_pred.append(predictions_full[predictions_full['RowId']<=max_row_full].copy())\n",
    "    new_pred = new_pred.sort_values(by=['RowId'])\n",
    "    new_pred.to_csv(\"combined_\"+file,index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
