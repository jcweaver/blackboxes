{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d230cf8de72e867e5717d0f5cf531d71189c7e5bd77bc2a42cd05122b296a561"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "##  Lenet5 Models -JackieN \n",
    "This File Produces A number of Lenet5 Models and Predictions based on varying degrees of cleaned Train data.\n",
    "\n",
    "Based on https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086 and \n",
    "\n",
    "https://deepai.org/publication/towards-good-practices-on-building-effective-cnn-baseline-model-for-person-re-identification#:~:text=The%20last%20key%20practice%20is%20to%20train%20CNN,based%20on%20the%20adaptive%20estimates%20of%20lower-order%20moments.\n",
    "\n",
    "The best score produced from the model using the clean data with all outliers removed is: 3.23581  \n",
    "\n",
    "Placing at position 72 on the leaderboard\n",
    "\n",
    "![](https://i.imgur.com/JwXgz3C.jpg)\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the utils path to point to the utils directory locally\n",
    "UTILS_PATH = \"c:/Users/mspuc/OneDrive/Berkeley/A - W207/Final/blackboxes/utils\"\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(UTILS_PATH)\n",
    "from load_models import LoadTrainModels\n",
    "from predict_models import PredictModels\n",
    "import imp\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model dir: C:/data/Jackie_Lenet5/\n"
     ]
    }
   ],
   "source": [
    "### this is gold right here.\n",
    "#import imp\n",
    "#imp.reload()\n",
    "\n",
    "#Clean file path\n",
    "file_path = \"C:/Data/CleanTrain_1/\"\n",
    "trainer = LoadTrainModels(\"C:/data/Jackie_Lenet5\", file_path)\n",
    "\n",
    "trainer.print_paths()"
   ]
  },
  {
   "source": [
    "### For every version of a clean Train file in a given path, create and save a model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Opening file:  clean_all_outliers.p\n",
      "Train Shape: (7041, 31)\n",
      "Begin model and train:\n",
      "Model name: clean_all_outliers_jn\n",
      "Scaling images\n",
      "Scaling complete.\n",
      "Number of images to be brightened: 7041\n",
      "Number of images to be dimmed: 7041\n",
      "Completed brighten and dim. Number of observations added to train: 14082\n",
      "Begining the split of Train with all features\n",
      "Looking for model JN\n",
      "Loading model: clean_all_outliers_jn\n",
      "Saving the history paramters file\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e581cea528b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhoizontal_flip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbrightness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saving the history paramters file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mhistory_param_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"_hparam.csv\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mdct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mhistory_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Get the files in the clean directory\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "#For every version of a cleaned Train file in CleanTrain directory, create and save a model\n",
    "for filename in files: \n",
    "    print(\"Opening file: \", filename)\n",
    "    clean_file = \"\".join((file_path,filename))\n",
    "    train_data = pickle.load( open( clean_file, \"rb\" ) )\n",
    "    train_data = train_data.drop(['level_0', 'check_sum', 'index'], axis=1,errors='ignore')\n",
    "    print(\"Train Shape:\", train_data.shape)\n",
    "\n",
    "    filename = str(filename).replace('.p', '').strip()\n",
    "    print(\"Begin model and train:\")\n",
    "    model_name = \"\".join((filename,\"_jn\"))\n",
    "    print(\"Model name:\", model_name)\n",
    "    model, history = trainer.train_model(model_name, train_data, hoizontal_flip = False,brightness = 1.4, dim = 0.3)\n",
    "    print(\"End model and train\")    \n",
    "    print()"
   ]
  },
  {
   "source": [
    "### For every model file in a given path, predict using the model and save the predictions in CSV file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model dir: C:/Data/Jackie_Lenet5/\n",
      "Pickle dir: C:/data/Predictions/\n",
      "Working with:  clean_all_outliers_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_all_outliers_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n",
      "Working with:  clean_duplicates_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_duplicates_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n",
      "Working with:  clean_o_dups_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_o_dups_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n",
      "Working with:  clean_o_outliers_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_o_outliers_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n",
      "Working with:  clean_wo_dups_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_wo_dups_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n",
      "Working with:  clean_w_dups_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_w_dups_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n",
      "Working with:  clean_w_outliers_Lenet5\n",
      "Begin Predict\n",
      "Scaling 1783 images...\n",
      "Scaling of 1783 observations complete.\n",
      "Begining the split of Test\n",
      "got unique ids\n",
      "test subset shape: (1783, 4)\n",
      "End with the split of Test\n",
      "(27124, 4)\n",
      "before melt: (1783, 30)\n",
      "after melt: (53490, 3)\n",
      "after merge: (27124, 2)\n",
      "C:/data/Predictions/clean_w_outliers_Lenet5Pred.csv\n",
      "Predictions written \n",
      "End model and train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Time to make some predictions\n",
    "\n",
    "id_lookup = pickle.load( open( \"../Data/id_lookup.p\", \"rb\" ) )\n",
    "test = pickle.load( open( \"../Data/test.p\", \"rb\" ) )\n",
    "\n",
    "#Using local paths as this is way faster...\n",
    "#Where the models are:\n",
    "file_path = \"C:/Data/Jackie_Lenet5\"\n",
    "#Where we want the predictions stored\n",
    "pred_path = \"C:/data/Predictions/\"\n",
    "\n",
    "predictor = PredictModels(file_path,pred_path , id_lookup)\n",
    "\n",
    "predictor.print_paths()\n",
    "\n",
    "files = os.listdir(file_path)\n",
    "#For every model in file_path, predict using the model and save the predictions in CSV file\n",
    "for filename in files:\n",
    "    if \".h5\" in filename:\n",
    "        base_name = filename[:-3]\n",
    "        model_json = ''.join((base_name,\".json\"))\n",
    "        print(\"Working with: \", base_name)\n",
    "        print(\"Begin Predict\")\n",
    "        Y= predictor.predict_lenet5(base_name, filename, model_json, test)\n",
    "        print(\"End model and train\")    \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}