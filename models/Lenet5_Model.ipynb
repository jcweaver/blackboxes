{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lenet5 Models -JackieN \n",
    "This File Produces A number of Lenet5 Models and Predictions based on varying degrees of cleaned Train data.\n",
    "\n",
    "Based on https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086 and \n",
    "\n",
    "https://deepai.org/publication/towards-good-practices-on-building-effective-cnn-baseline-model-for-person-re-identification#:~:text=The%20last%20key%20practice%20is%20to%20train%20CNN,based%20on%20the%20adaptive%20estimates%20of%20lower-order%20moments.\n",
    "\n",
    "The best score produced from the model using the clean data with all outliers removed is: 3.23581  \n",
    "\n",
    "Placing at position 72 on the leaderboard\n",
    "\n",
    "![](https://i.imgur.com/JwXgz3C.jpg)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from utils import predict_models, load_models, transform_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is gold right here.\n",
    "imp.reload(load_models)\n",
    "file_path = \"C:/Data/Augmented/\"\n",
    "trainer = load_models.LoadTrainModels(\"C:/data/Jackie_Lenet5_Aug\", file_path)\n",
    "\n",
    "trainer.print_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every version of a clean Train file in a given path, create and save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "imp.reload(load_models)\n",
    "\n",
    "file_path = \"C:/Data/Augmented/\"\n",
    "#files = os.listdir(\"../CleanTrain\")\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "#For every version of a cleaned Train file in CleanTrain directory, create and save a model\n",
    "for filename in files: \n",
    "    print(\"Opening file: \", filename)\n",
    "    clean_file = \"\".join((file_path,filename))\n",
    "    train_data = pickle.load( open( clean_file, \"rb\" ) )\n",
    "    train_data = train_data.drop(['level_0', 'check_sum', 'index'], axis=1,errors='ignore')\n",
    "    print(\"Train Shape:\", train_data.shape)\n",
    "\n",
    "    filename = str(filename).replace('.p', '').strip()\n",
    "    print(\"Begin model and train:\")\n",
    "    model_name = \"\".join((filename,\"_Lenet5\"))\n",
    "    print(\"Model name:\", model_name)\n",
    "    model, history = trainer.train_lenet5(model_name, train_data,verbose = True)\n",
    "    print(\"End model and train\")    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every model file in a given path, predict using the model and save the predictions in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp.reload(predict_models)\n",
    "imp.reload(transform_data)\n",
    "\n",
    "id_lookup = pickle.load( open( \"Data/id_lookup.p\", \"rb\" ) )\n",
    "test = pickle.load( open( \"Data/test.p\", \"rb\" ) )\n",
    "\n",
    "#Using local paths as this is way faster...\n",
    "file_path = \"C:/Data/Jackie_Lenet5_Aug\"\n",
    "pred_path = \"C:/data/Predictions_Aug/\"\n",
    "\n",
    "predictor = predict_models.PredictModels(file_path,pred_path , id_lookup)\n",
    "\n",
    "predictor.print_paths()\n",
    "\n",
    "files = os.listdir(file_path)\n",
    "#For every model in file_path, predict using the model and save the predictions in CSV file\n",
    "for filename in files:\n",
    "    if \".h5\" in filename:\n",
    "        base_name = filename[:-3]\n",
    "        model_json = ''.join((base_name,\".json\"))\n",
    "        print(\"Working with: \", base_name)\n",
    "        print(\"Begin Predict\")\n",
    "        Y= predictor.predict_lenet5(base_name, filename, model_json, test)\n",
    "        print(\"End model and train\")    \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Predictions using full columns vs separate predictions plus new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/predictions/\"\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "#All of the output rows for the submission file <= 17592 are keypoints for images with > 8 keypoints\n",
    "#All rows following this only require 6 or 8 keypoints\n",
    "max_row_full = 17592\n",
    "\n",
    "#For all the different prediction files, combine predictions from the 30/full model with the 8/separate model\n",
    "for file in files:\n",
    "    if \"Lenet5\" in file:\n",
    "        if \"flip\" in file:\n",
    "            print(file)\n",
    "            predictions_separate= pd.read_csv(\"data/predictions/\"+file)\n",
    "            file = file.replace(\"_flip\",\"\")\n",
    "            predictions_full = pd.read_csv(\"data/Predictions_l5/\"+file)\n",
    "            new_pred = predictions_separate[predictions_separate['RowId']>max_row_full].copy()\n",
    "            new_pred = new_pred.append(predictions_full[predictions_full['RowId']<=max_row_full].copy())\n",
    "            new_pred = new_pred.sort_values(by=['RowId'])\n",
    "            new_pred.to_csv(\"combined_\"+\"_flip\"+file,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
