{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import zlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "source": [
    "### Load Data For EDA\n",
    "There are 4 files found in the data directory:\n",
    "\n",
    "- IdLookupTable - TBD\n",
    "- SampleSubmission - TBD\n",
    "- test - TBD\n",
    "- training - TBD \n",
    "\n",
    "We must load this data in order to perform EDA.  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Begin file 'data/IdLookupTable.csv' load\n",
      "id_lookup\n",
      "\tEnd File load with shape: (27124, 4) \n",
      "\n",
      "Begin file 'data/SampleSubmission.csv' load\n",
      "sample_submission\n",
      "\tEnd File load with shape: (27124, 2) \n",
      "\n",
      "Begin file 'data/test.csv' load\n",
      "test\n",
      "\tProcessing 1783 images...\n",
      "\tEnd File load with shape: (1783, 2) \n",
      "\n",
      "Begin file 'data/training.csv' load\n",
      "train\n",
      "\tProcessing 7049 images...\n",
      "\tEnd File load with shape: (7049, 31) \n",
      "\n",
      "Data load complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://realpython.com/python-zip-function/#:~:text=%20Using%20the%20Python%20zip%20()%20Function%20for,zip%20()%20function%20works%20differently%20in...%20More\n",
    "\n",
    "df, git_path = {}, 'data/'\n",
    "for file_name, file_ref, n, t in zip(['IdLookupTable.csv', 'SampleSubmission.csv', 'test.csv', 'training.csv'],\n",
    "                        ['id_lookup', 'sample_submission', 'test', 'train'],\n",
    "                        [['row_id', 'image_id', 'feature_name', 'location'],\n",
    "                            ['row_id', 'location'],\n",
    "                            ['image_id', 'image'],\n",
    "                            ['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x', 'right_eye_center_y', 'left_eye_inner_corner_x', 'left_eye_inner_corner_y', \n",
    "                            'left_eye_outer_corner_x', 'left_eye_outer_corner_y', 'right_eye_inner_corner_x', 'right_eye_inner_corner_y', 'right_eye_outer_corner_x', \n",
    "                            'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y', \n",
    "                            'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y', \n",
    "                            'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x', 'mouth_center_top_lip_y', \n",
    "                            'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'image']],\n",
    "                            [{'row_id':'uint16', 'image_id':'uint16', 'location':'float32'},\n",
    "                            {'row_id':'uint16', 'location':'float32'},\n",
    "                            {'image_id':'uint16', 'image':'object'},\n",
    "                            {'left_eye_center_x':'float32', 'left_eye_center_y':'float32', 'right_eye_center_x':'float32', 'right_eye_center_y':'float32', \n",
    "                            'left_eye_inner_corner_x':'float32', 'left_eye_inner_corner_y':'float32', 'left_eye_outer_corner_x':'float32', 'left_eye_outer_corner_y':'float32', \n",
    "                            'right_eye_inner_corner_x':'float32', 'right_eye_inner_corner_y':'float32', 'right_eye_outer_corner_x':'float32', 'right_eye_outer_corner_y':'float32', \n",
    "                            'left_eyebrow_inner_end_x':'float32', 'left_eyebrow_inner_end_y':'float32', 'left_eyebrow_outer_end_x':'float32', 'left_eyebrow_outer_end_y':'float32', \n",
    "                            'right_eyebrow_inner_end_x':'float32', 'right_eyebrow_inner_end_y':'float32', 'right_eyebrow_outer_end_x':'float32', 'right_eyebrow_outer_end_y':'float32', \n",
    "                            'nose_tip_x':'float32', 'nose_tip_y':'float32', 'mouth_left_corner_x':'float32', 'mouth_left_corner_y':'float32', 'mouth_right_corner_x':'float32', \n",
    "                            'mouth_right_corner_y':'float32', 'mouth_center_top_lip_x':'float32', 'mouth_center_top_lip_y':'float32', 'mouth_center_bottom_lip_x':'float32', \n",
    "                            'mouth_center_bottom_lip_y':'float32', 'image':'object'}]):\n",
    "    print(\"Begin file '%s' load\" % \"\".join( (git_path, file_name)))\n",
    "    print(file_ref)\n",
    "    df[file_ref] = pd.read_csv(\"\".join( (git_path,file_name) ), names = n, dtype = t, skiprows = 1)\n",
    "    #If the file contains an image column like in the case of test.csv process those images now. \n",
    "    if \"image\" in df[file_ref]:\n",
    "        print(\"\\tProcessing %d images...\" % df[file_ref].shape[0])\n",
    "        #Get the row with the image data and store it in the dataframe \n",
    "        df[file_ref]['image'] = df[file_ref][\"image\"].map(lambda x: np.array(list(map(int, x.split(\" \")))))\n",
    "    print(\"\\tEnd File load with shape:\", df[file_ref].shape, \"\\n\")\n",
    "\n",
    "print(\"Data load complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df['train'][['image']], df['test'][['image']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EDA on duplicate data in train and test datasets: \nThe train dataset has 543 unique images and 1098 duplicate images\nThe test dataset has 29 unique images and 60 duplicate images\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate train images\n",
    "train = df['train'].reset_index().copy()\n",
    "#Get the images and perform a checksum on every image in train: https://www.geeksforgeeks.org/zlib-adler32-in-python/\n",
    "train['check_sum'] = train.image.map(lambda x: zlib.adler32(x))\n",
    "\n",
    "#Create a DF to store duplicates, grouping them together and sorting them\n",
    "train_duplicates = pd.DataFrame(train.groupby(by='check_sum').index.count().sort_values()).reset_index()\n",
    "#Add a column to keep track of how many of each check sum there are\n",
    "train_duplicates.columns = ['check_sum', 'number_found']\n",
    "#Keep the ones where we have > 1 number_found\n",
    "train_duplicates = train_duplicates[(train_duplicates.number_found > 1)]\n",
    "#Now do a left outer join back to train_duplicates.  This should only keep the duplicates \n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#:~:text=merge%20is%20a%20function%20in%20the%20pandas%20namespace,,the%20index-on-index%20(by%20default)%20and%20column%20(s)-on-index%20join.\n",
    "train_duplicates = pd.merge(train_duplicates, train[['index', 'check_sum']],  how = 'left', on=['check_sum']).sort_values(by=['number_found', 'check_sum'], ascending = False)\n",
    "\n",
    "\n",
    "#Now do the same for test:\n",
    "# Check for duplicate train images\n",
    "test = df['test'].reset_index().copy()\n",
    "#Get the images and perform a checksum on every image in train: https://www.geeksforgeeks.org/zlib-adler32-in-python/\n",
    "test['check_sum'] = test.image.map(lambda x: zlib.adler32(x))\n",
    "#Create a DF to store duplicates, grouping them together and sorting them\n",
    "test_duplicates = pd.DataFrame(test.groupby(by='check_sum').index.count().sort_values()).reset_index()\n",
    "#Add a column to keep track of how many of each check sum there are\n",
    "test_duplicates.columns = ['check_sum', 'number_found']\n",
    "#Keep the ones where we have > 1 number_found\n",
    "test_duplicates = test_duplicates[(test_duplicates.number_found > 1)]\n",
    "#Now do a left outer join back to train_duplicates.  This should only keep the duplicates \n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#:~:text=merge%20is%20a%20function%20in%20the%20pandas%20namespace,,the%20index-on-index%20(by%20default)%20and%20column%20(s)-on-index%20join.\n",
    "test_duplicates = pd.merge(test_duplicates, test[['index', 'check_sum']],  how = 'left', on=['check_sum']).sort_values(by=['number_found', 'check_sum'], ascending = False)\n",
    "\n",
    "print(\"EDA on duplicate data in train and test datasets: \")\n",
    "print(\"The train dataset has %d unique images and %d duplicate images\" % (len(np.unique(train_duplicates.check_sum)), len(train_duplicates)))\n",
    "print(\"The test dataset has %d unique images and %d duplicate images\" % (len(np.unique(test_duplicates.check_sum)),len(test_duplicates)))\n",
    "\n",
    "#Clean up:\n",
    "#We don't really need the check_sum column anymore...so drop it\n",
    "train.drop(columns=['check_sum'], inplace=True)\n",
    "test.drop(columns=['check_sum'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}